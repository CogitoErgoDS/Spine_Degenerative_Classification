{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import mlflow\n",
    "import mlflow.pytorch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    " # Import functions from the module\n",
    "import importlib\n",
    "import help_files._0_definitions \n",
    "import  help_files._1_visuals_script\n",
    "# import  help_files._01_load_data\n",
    " # Reload the module to apply the changes to the script\n",
    "importlib.reload(help_files._0_definitions)\n",
    "importlib.reload(help_files._1_visuals_script)\n",
    "# importlib.reload(help_files._01_load_data)\n",
    "import  help_files._1_visuals_script  as pauls_vs\n",
    "# Group by 'condition', 'level', and 'severity' and count occurrences\n",
    "from help_files._0_definitions import count_severity_by_condition_level \n",
    "# Define the path\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.width\", 1000)  # Set a large width to prevent line wrapping\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_definitions.py\") as file:\n",
    "    exec(file.read())\n",
    "    ### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_run_definitions.py\") as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames have been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "file_names = [\"train_df_3_cat.csv\", \"test_df_3_cat.csv\"]\n",
    "# Load the data from the CSV files\n",
    "dataframes = [pd.read_csv(data_path_vor / file_name) for file_name in file_names]\n",
    "# Unpack the dataframes into separate variables\n",
    "train_df, test_df = dataframes\n",
    "\n",
    "print(\"DataFrames have been loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:red\"> this is a small sample : 48692</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining small sample vs. end smaple\n",
    "whole_data_set = False\n",
    "# end sample or small sample    \n",
    "if whole_data_set == True:\n",
    "    print(\"Using the whole data set\")\n",
    "else:\n",
    "    train_df = train_df.sample(n=3, random_state=RSEED)\n",
    "    test_df = test_df.sample(n=6, random_state=RSEED)\n",
    "    display(Markdown('<span style=\"color:red\"> this is a small sample : 48692</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new code l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>severity</th>\n",
       "      <th>condition</th>\n",
       "      <th>level</th>\n",
       "      <th>series_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>image_path</th>\n",
       "      <th>missing_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>684747623</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>3.273038e+09</td>\n",
       "      <td>370.053272</td>\n",
       "      <td>506.149133</td>\n",
       "      <td>data/train_images_origin/684747623/774494956/7...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>2325650566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>2.076691e+09</td>\n",
       "      <td>293.720819</td>\n",
       "      <td>472.222032</td>\n",
       "      <td>data/train_images_origin/2325650566/20477869/5...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>624881903</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.233162e+09</td>\n",
       "      <td>375.776524</td>\n",
       "      <td>475.359848</td>\n",
       "      <td>data/train_images_origin/624881903/1233161684/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        study_id  severity  condition  level     series_id           x           y                                         image_path  missing_image\n",
       "1935   684747623       2.0          0  l5/s1  3.273038e+09  370.053272  506.149133  data/train_images_origin/684747623/774494956/7...          False\n",
       "6494  2325650566       1.0          0  l5/s1  2.076691e+09  293.720819  472.222032  data/train_images_origin/2325650566/20477869/5...          False\n",
       "1720   624881903       2.0          0  l5/s1  1.233162e+09  375.776524  475.359848  data/train_images_origin/624881903/1233161684/...          False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define U-Net model with reduced complexity\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Downsampling (Encoder) with reduced filter sizes\n",
    "        self.enc1 = self.conv_block(in_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 256)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # Upsampling (Decoder)\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up3 = self.upconv_block(256, 128)\n",
    "        self.up2 = self.upconv_block(128, 64)\n",
    "        self.up1 = self.upconv_block(64, 32)\n",
    "\n",
    "        # Final convolution for regression\n",
    "        self.final_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "\n",
    "        # Decoder path with upsampling and skip connections\n",
    "        up4 = F.interpolate(self.up4(bottleneck), size=enc4.shape[2:]) + enc4\n",
    "        up3 = F.interpolate(self.up3(up4), size=enc3.shape[2:]) + enc3\n",
    "        up2 = F.interpolate(self.up2(up3), size=enc2.shape[2:]) + enc2\n",
    "        up1 = F.interpolate(self.up1(up2), size=enc1.shape[2:]) + enc1\n",
    "\n",
    "        # Final output\n",
    "        output = self.final_conv(up1)\n",
    "        output = F.adaptive_avg_pool2d(output, (1, 1))  # Reduce spatial dimensions\n",
    "        output = output.view(output.size(0), -1)  # Reshape to [batch_size, 2]\n",
    "        return output\n",
    "\n",
    "# Custom dataset for MRI localization\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        try:\n",
    "            dicom_image = pydicom.dcmread(img_path)\n",
    "            image_array = dicom_image.pixel_array\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load DICOM file at {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        image = Image.fromarray(image_array)\n",
    "        # Convert grayscale to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        x = torch.tensor(self.data.iloc[idx]['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[idx]['y'], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.tensor([x, y])\n",
    "\n",
    "# Data transformations (smaller image size of 128x128)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# DataLoader setup\n",
    "dataset = MRILocalizationDataset(data=train_df, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Model initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(in_channels=3, out_channels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set experiment in MLflow\n",
    "experiment_name = \"MRI_Localization_UNet\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Define an input example for MLflow\n",
    "input_example = torch.randn(1, 3, 224, 224, device=device)  # Random tensor simulating one sample image\n",
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 5  # Number of epochs to wait for improvement before stopping\n",
    "diverge_count = 0\n",
    "max_diverge_count = 3  # Number of times validation loss can diverge before stopping\n",
    "stop_threshold = 0.1  # Threshold for validation loss divergence\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log experiment parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"model_architecture\", \"U-Net for Localization\")\n",
    "    mlflow.log_param(\"output_coordinates\", \"x, y\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            if inputs is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "\n",
    "        # Validation loss calculation (this should happen after training)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                if inputs is None or targets is None:\n",
    "                    continue\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Check if validation loss improves and save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter if validation loss improves\n",
    "            best_model_path = f\"best_model_weights_epoch_{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            mlflow.log_artifact(best_model_path)  # Log the best model weights\n",
    "            os.remove(best_model_path)  # Optionally, delete the local file after logging\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to lack of validation loss improvement.\")\n",
    "                break\n",
    "\n",
    "        # Early stopping based on validation loss divergence (optional)\n",
    "        if val_loss > best_val_loss * (1 + stop_threshold):\n",
    "            diverge_count += 1\n",
    "            if diverge_count >= max_diverge_count:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to validation loss divergence.\")\n",
    "                break\n",
    "        else:\n",
    "            diverge_count = 0  # Reset diverge count if validation loss doesn't diverge\n",
    "\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}')            \n",
    "\n",
    "        # Log metrics at each epoch\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "    # Log the final model\n",
    "    input_example_np = input_example.cpu().numpy()  # Convert tensor to numpy array\n",
    "    mlflow.pytorch.log_model(model, \"final_model\", input_example=input_example_np)\n",
    "\n",
    "\n",
    "    # Plot and save the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(num_epochs), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    mlflow.log_artifact(\"loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define U-Net model with reduced complexity\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Downsampling (Encoder) with reduced filter sizes\n",
    "        self.enc1 = self.conv_block(in_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 256)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # Upsampling (Decoder)\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up3 = self.upconv_block(256, 128)\n",
    "        self.up2 = self.upconv_block(128, 64)\n",
    "        self.up1 = self.upconv_block(64, 32)\n",
    "\n",
    "        # Final convolution for regression\n",
    "        self.final_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "\n",
    "        # Decoder path with upsampling and skip connections\n",
    "        up4 = F.interpolate(self.up4(bottleneck), size=enc4.shape[2:]) + enc4\n",
    "        up3 = F.interpolate(self.up3(up4), size=enc3.shape[2:]) + enc3\n",
    "        up2 = F.interpolate(self.up2(up3), size=enc2.shape[2:]) + enc2\n",
    "        up1 = F.interpolate(self.up1(up2), size=enc1.shape[2:]) + enc1\n",
    "\n",
    "        # Final output\n",
    "        output = self.final_conv(up1)\n",
    "        output = F.adaptive_avg_pool2d(output, (1, 1))  # Reduce spatial dimensions\n",
    "        output = output.view(output.size(0), -1)  # Reshape to [batch_size, 2]\n",
    "        return output\n",
    "\n",
    "# Custom dataset for MRI localization\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        try:\n",
    "            dicom_image = pydicom.dcmread(img_path)\n",
    "            image_array = dicom_image.pixel_array\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load DICOM file at {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        image = Image.fromarray(image_array)\n",
    "        # Convert grayscale to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        x = torch.tensor(self.data.iloc[idx]['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[idx]['y'], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.tensor([x, y])\n",
    "\n",
    "# Data transformations (smaller image size of 128x128)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 8\n",
    "num_epochs =3\n",
    "\n",
    "# DataLoader setup\n",
    "dataset = MRILocalizationDataset(data=train_df, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 18\n",
    "num_epochs = 2\n",
    "learning_rate = 0.0001\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Model initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(in_channels=3, out_channels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set experiment in MLflow\n",
    "experiment_name = \"MRI_Localization_UNet\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Define an input example for MLflow\n",
    "input_example = torch.randn(1, 3, 224, 224, device=device)  # Random tensor simulating one sample image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#  6 minutes×30=180 minutes(or 3 hours per epoch für 100 )\n",
    "print(0.001 * 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/12 20:17:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 5  # Number of epochs to wait for improvement before stopping\n",
    "diverge_count = 0\n",
    "max_diverge_count = 3  # Number of times validation loss can diverge before stopping\n",
    "stop_threshold = 0.1  # Threshold for validation loss divergence\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log experiment parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"model_architecture\", \"U-Net for Localization\")\n",
    "    mlflow.log_param(\"output_coordinates\", \"x, y\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            if inputs is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "\n",
    "        # Validation loss calculation (this should happen after training)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                if inputs is None or targets is None:\n",
    "                    continue\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Check if validation loss improves and save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter if validation loss improves\n",
    "            best_model_path = f\"best_model_weights_epoch_{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            mlflow.log_artifact(best_model_path)  # Log the best model weights\n",
    "            os.remove(best_model_path)  # Optionally, delete the local file after logging\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to lack of validation loss improvement.\")\n",
    "                break\n",
    "\n",
    "        # Early stopping based on validation loss divergence (optional)\n",
    "        if val_loss > best_val_loss * (1 + stop_threshold):\n",
    "            diverge_count += 1\n",
    "            if diverge_count >= max_diverge_count:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to validation loss divergence.\")\n",
    "                break\n",
    "        else:\n",
    "            diverge_count = 0  # Reset diverge count if validation loss doesn't diverge\n",
    "\n",
    "        # Log metrics at each epoch\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "    # Save the final model\n",
    "    mlflow.pytorch.log_model(model, \"final_model\")\n",
    "\n",
    "    # Plot and save the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(num_epochs), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    mlflow.log_artifact(\"loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mss\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ss' is not defined"
     ]
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define U-Net model with reduced complexity\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Downsampling (Encoder) with reduced filter sizes\n",
    "        self.enc1 = self.conv_block(in_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 256)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # Upsampling (Decoder)\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up3 = self.upconv_block(256, 128)\n",
    "        self.up2 = self.upconv_block(128, 64)\n",
    "        self.up1 = self.upconv_block(64, 32)\n",
    "\n",
    "        # Final convolution for regression\n",
    "        self.final_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "\n",
    "        # Decoder path with upsampling and skip connections\n",
    "        up4 = F.interpolate(self.up4(bottleneck), size=enc4.shape[2:]) + enc4\n",
    "        up3 = F.interpolate(self.up3(up4), size=enc3.shape[2:]) + enc3\n",
    "        up2 = F.interpolate(self.up2(up3), size=enc2.shape[2:]) + enc2\n",
    "        up1 = F.interpolate(self.up1(up2), size=enc1.shape[2:]) + enc1\n",
    "\n",
    "        # Final output\n",
    "        output = self.final_conv(up1)\n",
    "        output = F.adaptive_avg_pool2d(output, (1, 1))  # Reduce spatial dimensions\n",
    "        output = output.view(output.size(0), -1)  # Reshape to [batch_size, 2]\n",
    "        return output\n",
    "\n",
    "# Custom dataset for MRI localization\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        try:\n",
    "            dicom_image = pydicom.dcmread(img_path)\n",
    "            image_array = dicom_image.pixel_array\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load DICOM file at {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        image = Image.fromarray(image_array)\n",
    "        # Convert grayscale to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        x = torch.tensor(self.data.iloc[idx]['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[idx]['y'], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.tensor([x, y])\n",
    "\n",
    "# Data transformations (smaller image size of 128x128)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "# DataLoader setup\n",
    "train_dataset = MRILocalizationDataset(data=train_df, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Model initialization\n",
    "model = UNet(in_channels=3, out_channels=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set experiment in MLflow\n",
    "experiment_name = \"MRI_Localization_UNet\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log experiment parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"model_architecture\", \"U-Net for Localization\")\n",
    "    mlflow.log_param(\"output_coordinates\", \"x, y\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            if inputs is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with disabled gradient tracking\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_image = Image.open(\"path_to_new_image.dcm\")\n",
    "    test_image = transform(test_image).unsqueeze(0)  # Add batch dimension\n",
    "    predicted_coords = model(test_image)\n",
    "    print(f\"Predicted x, y coordinates: {predicted_coords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mss\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ss' is not defined"
     ]
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Downsampling (Encoder)\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "\n",
    "        # Upsampling (Decoder)\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
    "        self.up3 = self.upconv_block(512, 256)\n",
    "        self.up2 = self.upconv_block(256, 128)\n",
    "        self.up1 = self.upconv_block(128, 64)\n",
    "\n",
    "\n",
    "\n",
    "        # Final convolution for regression\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)  # Now outputs 2 values (x, y)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.enc1(x)\n",
    "        print(f\"enc1 shape: {enc1.shape}\")\n",
    "        enc2 = self.enc2(enc1)\n",
    "        print(f\"enc2 shape: {enc2.shape}\")\n",
    "        enc3 = self.enc3(enc2)\n",
    "        print(f\"enc3 shape: {enc3.shape}\")\n",
    "        enc4 = self.enc4(enc3)\n",
    "        print(f\"enc4 shape: {enc4.shape}\")\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "        print(f\"bottleneck shape: {bottleneck.shape}\")\n",
    "\n",
    "        # Decoder path with upsampling and skip connections\n",
    "        up4 = self.up4(bottleneck)\n",
    "        print(f\"up4 (before interpolation) shape: {up4.shape}\")\n",
    "        up4 = F.interpolate(up4, size=enc4.shape[2:])  # Resize up4 to match enc4\n",
    "        print(f\"up4 (after interpolation) shape: {up4.shape}\")\n",
    "        up4 = up4 + enc4  # Skip connection\n",
    "        print(f\"up4 (after addition with enc4) shape: {up4.shape}\")\n",
    "\n",
    "        up3 = self.up3(up4)\n",
    "        print(f\"up3 (before interpolation) shape: {up3.shape}\")\n",
    "        up3 = F.interpolate(up3, size=enc3.shape[2:])  # Resize to match enc3\n",
    "        print(f\"up3 (after interpolation) shape: {up3.shape}\")\n",
    "        up3 = up3 + enc3  # Skip connection\n",
    "        print(f\"up3 (after addition with enc3) shape: {up3.shape}\")\n",
    "\n",
    "        up2 = self.up2(up3)\n",
    "        print(f\"up2 (before interpolation) shape: {up2.shape}\")\n",
    "        up2 = F.interpolate(up2, size=enc2.shape[2:])  # Resize to match enc2\n",
    "        print(f\"up2 (after interpolation) shape: {up2.shape}\")\n",
    "        up2 = up2 + enc2  # Skip connection\n",
    "        print(f\"up2 (after addition with enc2) shape: {up2.shape}\")\n",
    "\n",
    "        up1 = self.up1(up2)\n",
    "        print(f\"up1 (before interpolation) shape: {up1.shape}\")\n",
    "        up1 = F.interpolate(up1, size=enc1.shape[2:])  # Resize to match enc1\n",
    "        print(f\"up1 (after interpolation) shape: {up1.shape}\")\n",
    "        up1 = up1 + enc1  # Skip connection\n",
    "        print(f\"up1 (after addition with enc1) shape: {up1.shape}\")\n",
    "\n",
    "        # Final output\n",
    "        output = self.final_conv(up1)\n",
    "\n",
    "        # Global average pooling to reduce spatial dimensions for coordinate regression\n",
    "        output = F.adaptive_avg_pool2d(output, (1, 1))  # output shape: [batch_size, 2, 1, 1]\n",
    "        output = output.view(output.size(0), -1)  # Reshape to [batch_size, 2]\n",
    "        print(f\"output shape: {output.shape}\")\n",
    "        return output\n",
    "\n",
    "\n",
    "# Custom dataset for MRI localization\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        dicom_image = pydicom.dcmread(img_path)\n",
    "        image_array = dicom_image.pixel_array\n",
    "        image = Image.fromarray(image_array)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Extract x and y coordinates for localization\n",
    "        x = torch.tensor(self.data.iloc[idx]['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[idx]['y'], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.tensor([x, y]) \n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Assuming 'df' is a pandas DataFrame containing MRI data\n",
    "train_dataset = MRILocalizationDataset(data=train_df, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "model = UNet(in_channels=3, out_channels=2)  # 3 channels for RGB, 2 for x and y coordinates\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for coordinate regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Save model with MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "    mlflow.log_params({\"epochs\": epochs, \"batch_size\": 4, \"learning_rate\": 1e-4})\n",
    "\n",
    "# Inference (predicting x, y coordinates for new test images)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_image = Image.open(\"path_to_new_image.dcm\")\n",
    "    test_image = transform(test_image).unsqueeze(0)  # Add batch dimension\n",
    "    predicted_coords = model(test_image)\n",
    "    print(f\"Predicted x, y coordinates: {predicted_coords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sssss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calss definition dataloader (do not change over models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "\n",
    "# Define the U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=2):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoding path (downsampling)\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.center = self.conv_block(512, 1024)\n",
    "\n",
    "        # Decoding path (upsampling)\n",
    "        self.upconv4 = self.upconv_block(1024 + 512, 512)  # Concatenate 1024 + 512\n",
    "        self.upconv3 = self.upconv_block(512 + 256, 256)   # Concatenate 512 + 256\n",
    "        self.upconv2 = self.upconv_block(256 + 128, 128)   # Concatenate 256 + 128\n",
    "        self.upconv1 = self.upconv_block(128 + 64, 64)     # Concatenate 128 + 64\n",
    "\n",
    "        # Final convolution to match the output size\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        # Use F.interpolate for resizing to the correct size\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),  # Reduce channels\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding path\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n",
    "        center = self.center(F.max_pool2d(enc4, 2))\n",
    "\n",
    "        # Decoding path (upsampling)\n",
    "        up4 = self.upconv4(torch.cat([center, enc4], 1))  # Concatenating with enc4\n",
    "        up4 = F.interpolate(up4, size=enc4.shape[2:], mode='bilinear', align_corners=False)\n",
    "        up3 = self.upconv3(torch.cat([up4, enc3], 1))  # Concatenating with enc3\n",
    "        up3 = F.interpolate(up3, size=enc3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        up2 = self.upconv2(torch.cat([up3, enc2], 1))  # Concatenating with enc2\n",
    "        up2 = F.interpolate(up2, size=enc2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        up1 = self.upconv1(torch.cat([up2, enc1], 1))  # Concatenating with enc1\n",
    "        up1 = F.interpolate(up1, size=enc1.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Final output\n",
    "        out = self.final_conv(up1)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "dataset = MRILocalizationDataset(data=train_df, transform=transform)\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloader\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(in_channels=3, out_channels=2)  # For x and y coordinates\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# MLflow setup\n",
    "experiment_name = \"MRI_Localization_UNet\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "stop_threshold = 0.4\n",
    "max_diverge_count = 3\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "diverge_count = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"model_architecture\", \"U-Net for Localization\")\n",
    "    mlflow.log_param(\"output_coordinates\", \"x, y\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, coordinates in train_loader:\n",
    "            images, coordinates = images.to(device), coordinates.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Flatten the output tensor to match the target shape (batch_size, 2)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "\n",
    "            # Ensure the shape of outputs matches the target (x, y)\n",
    "            if outputs.size(1) != 2:\n",
    "                outputs = outputs[:, :2]  # Only take the first 2 channels if the output size is incorrect\n",
    "\n",
    "            loss = criterion(outputs, coordinates)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, coordinates in val_loader:\n",
    "                images, coordinates = images.to(device), coordinates.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Flatten the output tensor to match the target shape (batch_size, 2)\n",
    "                outputs = outputs.view(outputs.size(0), -1)\n",
    "                if outputs.size(1) != 2:\n",
    "                    outputs = outputs[:, :2]\n",
    "\n",
    "                loss = criterion(outputs, coordinates)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Check if validation loss improves and save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter if validation loss improves\n",
    "            best_model_path = f\"best_model_weights_epoch_{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            mlflow.log_artifact(best_model_path)  # Log the best model weights\n",
    "            os.remove(best_model_path)  # Optionally, delete the local file after logging\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to lack of validation loss improvement.\")\n",
    "                break\n",
    "\n",
    "        # Early stopping based on validation loss divergence (optional)\n",
    "        if val_loss > best_val_loss * (1 + stop_threshold):\n",
    "            diverge_count += 1\n",
    "            if diverge_count >= max_diverge_count:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to validation loss divergence.\")\n",
    "                break\n",
    "        else:\n",
    "            diverge_count = 0  # Reset diverge count if validation loss doesn't diverge\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save model at each epoch (optional)\n",
    "        epoch_model_path = f\"model_epoch_{epoch + 1}.pt\"\n",
    "        torch.save(model.state_dict(), epoch_model_path)\n",
    "        mlflow.log_artifact(epoch_model_path)  # Log the model weights at each epoch\n",
    "        os.remove(epoch_model_path)  # Optionally, delete the local file after logging\n",
    "\n",
    "    # Plot and save loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(num_epochs), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    mlflow.log_artifact(\"loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "\n",
    "# Define U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Downsampling (Encoder)\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "        \n",
    "        # Upsampling (Decoder)\n",
    "        self.up4 = self.upconv_block(1024, 512)\n",
    "        self.up3 = self.upconv_block(512, 256)\n",
    "        self.up2 = self.upconv_block(256, 128)\n",
    "        self.up1 = self.upconv_block(128, 64)\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Downsampling\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "        \n",
    "        # Upsampling\n",
    "        up4 = self.up4(bottleneck)\n",
    "        up4 = up4 + enc4  # Skip connection\n",
    "        up3 = self.up3(up4)\n",
    "        up3 = up3 + enc3  # Skip connection\n",
    "        up2 = self.up2(up3)\n",
    "        up2 = up2 + enc2  # Skip connection\n",
    "        up1 = self.up1(up2)\n",
    "        up1 = up1 + enc1  # Skip connection\n",
    "        \n",
    "        # Final output layer\n",
    "        output = self.final_conv(up1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Custom dataset for MRI localization\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        dicom_image = pydicom.dcmread(img_path)\n",
    "        image_array = dicom_image.pixel_array\n",
    "        image = Image.fromarray(image_array)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Extract x and y coordinates for localization\n",
    "        x = torch.tensor(self.data.iloc[idx]['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[idx]['y'], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.tensor([x, y]) \n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Assuming 'df' is a pandas DataFrame containing MRI data\n",
    "train_dataset = MRILocalizationDataset(data=df, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "model = UNet(in_channels=3, out_channels=2)  # 3 channels for RGB, 2 for x and y coordinates\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for coordinate regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Save model with MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "    mlflow.log_params({\"epochs\": epochs, \"batch_size\": 4, \"learning_rate\": 1e-4})\n",
    "\n",
    "# Inference (predicting x, y coordinates for new test images)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_image = Image.open(\"path_to_new_image.dcm\")\n",
    "    test_image = transform(test_image).unsqueeze(0)  # Add batch dimension\n",
    "    predicted_coords = model(test_image)\n",
    "    print(f\"Predicted x, y coordinates: {predicted_coords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sssss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: U-Net Model and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "\n",
    "# Define the U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Downsampling (Encoder)\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "        \n",
    "        # Upsampling (Decoder)\n",
    "        self.up4 = self.upconv_block(1024, 512)\n",
    "        self.up3 = self.upconv_block(512, 256)\n",
    "        self.up2 = self.upconv_block(256, 128)\n",
    "        self.up1 = self.upconv_block(128, 64)\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Downsampling\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "        \n",
    "        # Upsampling\n",
    "        up4 = self.up4(bottleneck)\n",
    "        up4 = up4 + enc4  # Skip connection\n",
    "        up3 = self.up3(up4)\n",
    "        up3 = up3 + enc3  # Skip connection\n",
    "        up2 = self.up2(up3)\n",
    "        up2 = up2 + enc2  # Skip connection\n",
    "        up1 = self.up1(up2)\n",
    "        up1 = up1 + enc1  # Skip connection\n",
    "        \n",
    "        # Final output layer\n",
    "        output = self.final_conv(up1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Dataset class for MRI Localization\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        dicom_image = pydicom.dcmread(img_path)\n",
    "        image_array = dicom_image.pixel_array\n",
    "        image = Image.fromarray(image_array)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        x = torch.tensor(self.data.iloc[idx]['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[idx]['y'], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.tensor([x, y])\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "dataset = MRILocalizationDataset(data=train_df, transform=transform)\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloader\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(in_channels=3, out_channels=2)  # For x and y coordinates\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# MLflow setup\n",
    "experiment_name = \"MRI_Localization_UNet\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "stop_threshold = 0.4\n",
    "max_diverge_count = 3\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "diverge_count = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"model_architecture\", \"U-Net for Localization\")\n",
    "    mlflow.log_param(\"output_coordinates\", \"x, y\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, coordinates in train_loader:\n",
    "            images, coordinates = images.to(device), coordinates.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Flatten the output tensor to match the target shape (batch_size, 2)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "\n",
    "            # Ensure the shape of outputs matches the target (x, y)\n",
    "            if outputs.size(1) != 2:\n",
    "                outputs = outputs[:, :2]  # Only take the first 2 channels if the output size is incorrect\n",
    "\n",
    "            loss = criterion(outputs, coordinates)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, coordinates in val_loader:\n",
    "                images, coordinates = images.to(device), coordinates.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Flatten the output tensor to match the target shape (batch_size, 2)\n",
    "                outputs = outputs.view(outputs.size(0), -1)\n",
    "                if outputs.size(1) != 2:\n",
    "                    outputs = outputs[:, :2]\n",
    "\n",
    "                loss = criterion(outputs, coordinates)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Check if validation loss improves and save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter if validation loss improves\n",
    "            best_model_path = f\"best_model_weights_epoch_{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            mlflow.log_artifact(best_model_path)  # Log the best model weights\n",
    "            os.remove(best_model_path)  # Optionally, delete the local file after logging\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to lack of validation loss improvement.\")\n",
    "                break\n",
    "\n",
    "        # Early stopping based on validation loss divergence (optional)\n",
    "        if val_loss > best_val_loss * (1 + stop_threshold):\n",
    "            diverge_count += 1\n",
    "            if diverge_count >= max_diverge_count:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to validation loss divergence.\")\n",
    "                break\n",
    "        else:\n",
    "            diverge_count = 0  # Reset diverge count if validation loss doesn't diverge\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save model at each epoch (optional)\n",
    "        epoch_model_path = f\"model_epoch_{epoch + 1}.pt\"\n",
    "        torch.save(model.state_dict(), epoch_model_path)\n",
    "        mlflow.log_artifact(epoch_model_path)  # Log the model weights at each epoch\n",
    "        os.remove(epoch_model_path)  # Optionally, delete the local file after logging\n",
    "\n",
    "\n",
    "    # Plot and save loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(50), train_losses, label='Train Loss')\n",
    "    plt.plot(range(50), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    mlflow.log_artifact(\"loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test predicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = test_df.drop(['severity', 'condition', 'level', 'series_id', 'missing_image'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.to_string(index=False, header=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import mlflow\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the trained model from MLflow\n",
    "model_path = \"C:/Users/HP1/Desktop/Spiced/capstone-project/mlruns/491281383988954356/344ed7741b8b4cb9bf87844ab1e511af/artifacts/final_model\"\n",
    "model = mlflow.pytorch.load_model(model_path)\n",
    "\n",
    "\n",
    "# Print the entire model architecture\n",
    "print(model)\n",
    "for name, layer in model.named_children():\n",
    "    print(f\"Layer name: {name}\")\n",
    "    print(layer)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
