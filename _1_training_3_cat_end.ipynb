{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FklhSI0Gg9R"
   },
   "source": [
    "Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    " # Import functions from the module\n",
    "import importlib\n",
    "import help_files._0_definitions \n",
    "# import  help_files._1_visuals_script\n",
    "# import  help_files._01_load_data\n",
    " # Reload the module to apply the changes to the script\n",
    "importlib.reload(help_files._0_definitions)\n",
    " \n",
    "# importlib.reload(help_files._01_load_data)\n",
    " \n",
    "# Group by 'condition', 'level', and 'severity' and count occurrences\n",
    "from help_files._0_definitions import count_severity_by_condition_level \n",
    "# Define the path\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.width\", 1000)  # Set a large width to prevent line wrapping\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_definitions.py\") as file:\n",
    "    exec(file.read())\n",
    "    ### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_run_definitions.py\") as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "file_names = [\"train_df_4_loc.csv\", \"test_df_4_loc.csv\"]\n",
    "# Load the data from the CSV files\n",
    "dataframes = [pd.read_csv(data_path_vor / file_name) for file_name in file_names]\n",
    "# Unpack the dataframes into separate variables\n",
    "train_df, test_df = dataframes\n",
    "\n",
    "print(\"DataFrames have been loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # end sample or small sample    \n",
    "if whole_data_set == False:\n",
    "    print(\"Using the whole data set\")\n",
    "else:\n",
    "    train_df = train_df.sample(n=50, random_state=RSEED)\n",
    "    test_df = test_df.sample(n=100, random_state=RSEED)\n",
    "    display(Markdown('<span style=\"color:red\"> this is a small sample : 48692</span>'))\n",
    "    train_df.dtypes\n",
    "train_df['condition'] = train_df['condition'].astype('category').cat.codes\n",
    "train_data = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['image_path'] = train_df['image_path'].apply(lambda p: os.path.normpath(p))\n",
    "\n",
    "import pydicom\n",
    "\n",
    "test_path = train_df['image_path'].iloc[0]  # Get the first path\n",
    "print(\"Testing file:\", test_path)\n",
    "\n",
    "try:\n",
    "    dicom_image = pydicom.dcmread(test_path)\n",
    "    print(\"File loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['severity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['condition'] = train_df['condition'].astype('category').cat.codes\n",
    "train_data = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Function to load DICOM images and convert them to RGB\n",
    "def load_dicom_image(image_path):\n",
    "    \"\"\"Load a DICOM image and convert it to an RGB image using pydicom and OpenCV.\"\"\"\n",
    "    dicom_data = pydicom.dcmread(image_path)\n",
    "    image_array = dicom_data.pixel_array\n",
    "\n",
    "    # Normalize the pixel data\n",
    "    image_array = image_array.astype(float)\n",
    "    image_array = (image_array / image_array.max() * 255).astype('uint8')\n",
    "\n",
    "    # Convert to RGB if grayscale\n",
    "    if len(image_array.shape) == 2:  # Grayscale image\n",
    "        image = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)\n",
    "    else:  # Already RGB\n",
    "        image = image_array\n",
    "\n",
    "    # Convert numpy array to PIL Image\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing image paths, x, y, and severity labels.\n",
    "            transform (callable, optional): Transformations to apply to the images.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        image_path = row['image_path']\n",
    "        x = row['x']\n",
    "        y = row['y']\n",
    "        severity = row['severity']\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = load_dicom_image(image_path)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, (x, y), severity\n",
    "\n",
    "\n",
    "# Localization Model\n",
    "class LocalizationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocalizationModel, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Output (x, y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# Classification Model\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)  # Output severity\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# Main function for automation\n",
    "def main(train_df):\n",
    "    # Check if the required columns exist\n",
    "    required_columns = ['study_id', 'severity', 'condition', 'level', 'series_id', 'x', 'y', 'image_path', 'missing_image', 'study_id_count']\n",
    "    \n",
    "    if not all(col in train_df.columns for col in required_columns):\n",
    "        raise ValueError(\"DataFrame is missing required columns.\")\n",
    "\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to ResNet input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "\n",
    "    # Create Dataset and DataLoader\n",
    "    train_dataset = MRILocalizationDataset(dataframe=train_df, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Instantiate models\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    localization_model = LocalizationModel().to(device)\n",
    "    classification_model = ClassificationModel(num_classes=3).to(device)  # Adjust for severity levels\n",
    "\n",
    "    # Loss functions and optimizers\n",
    "    criterion_localization = nn.MSELoss()\n",
    "    criterion_classification = nn.CrossEntropyLoss()\n",
    "    optimizer_localization = torch.optim.Adam(localization_model.parameters(), lr=0.001)\n",
    "    optimizer_classification = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        localization_model.train()\n",
    "        classification_model.train()\n",
    "\n",
    "        running_loss_localization = 0.0\n",
    "        running_loss_classification = 0.0\n",
    "\n",
    "        for images, (x_coords, y_coords), severities in train_loader:\n",
    "            images = images.to(device)\n",
    "            x_coords = torch.tensor(x_coords, dtype=torch.float32).to(device)\n",
    "            y_coords = torch.tensor(y_coords, dtype=torch.float32).to(device)\n",
    "            severities = torch.tensor(severities, dtype=torch.long).to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer_localization.zero_grad()\n",
    "            optimizer_classification.zero_grad()\n",
    "\n",
    "            # Localization model forward pass\n",
    "            predicted_coords = localization_model(images)\n",
    "            loss_localization = criterion_localization(predicted_coords, torch.stack((x_coords, y_coords), dim=1))\n",
    "            loss_localization.backward()\n",
    "\n",
    "            # Classification model forward pass\n",
    "            predicted_severity = classification_model(images)\n",
    "            loss_classification = criterion_classification(predicted_severity, severities)\n",
    "            loss_classification.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_localization.step()\n",
    "            optimizer_classification.step()\n",
    "\n",
    "            # Track losses\n",
    "            running_loss_localization += loss_localization.item()\n",
    "            running_loss_classification += loss_classification.item()\n",
    "\n",
    "        # Print epoch loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Localization Loss: {running_loss_localization/len(train_loader):.4f}, \"\n",
    "              f\"Classification Loss: {running_loss_classification/len(train_loader):.4f}\")\n",
    "\n",
    "    print(\"Training Complete\")\n",
    "\n",
    "    # Save the models\n",
    "    torch.save(localization_model.state_dict(), 'localization_model.pth')\n",
    "    torch.save(classification_model.state_dict(), 'classification_model.pth')\n",
    "\n",
    "    # Inference Example\n",
    "    localization_model.eval()\n",
    "    classification_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, (x_coords, y_coords), severities in train_loader:\n",
    "            images = images.to(device)\n",
    "            predicted_coords = localization_model(images)\n",
    "            predicted_severity = classification_model(images)\n",
    "\n",
    "            # Output results\n",
    "            print(f\"Predicted Coordinates: {predicted_coords}\")\n",
    "            print(f\"Predicted Severity: {predicted_severity}\")\n",
    "\n",
    "\n",
    "# Run the main function automatically\n",
    "if __name__ == \"__main__\":\n",
    "    # Load DataFrame from a CSV file (Replace with your actual file)\n",
    "    train_df = train_df  # Make sure to provide the correct path to your CSV file\n",
    "\n",
    "    # Automatically run the main function\n",
    "    main(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Step 1: Define the model architecture (same as training)\n",
    "model_for_prediciton = models.resnet50()\n",
    "model_for_prediciton.fc = torch.nn.Linear(model_for_prediciton.fc.in_features, 3)  # Update for your classes\n",
    "\n",
    "# Step 2: Load the best model weights\n",
    "# Adjust this path if necessary\n",
    "best_weights_path = \"C:/Users/HP1/Desktop/Spiced/capstone-project/mlruns/711328181724227740/deaa903e488043d19aad572a6c429479/artifacts/best_model_weights_epoch_1.pt\"\n",
    "model_for_prediciton.load_state_dict(torch.load(best_weights_path))\n",
    "\n",
    "# Step 3: Set the model to evaluation mode\n",
    "model_for_prediciton.eval()\n",
    "model_for_prediciton.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <run_id> with your actual run ID and <path_to_model> with the artifact path used during logging\n",
    "model_path = r\"C:\\Users\\HP1\\Desktop\\Spiced\\capstone-project\\mlruns\\711328181724227740\\deaa903e488043d19aad572a6c429479\\artifacts\\final_model\"\n",
    "model_for_prediciton = mlflow.pytorch.load_model(model_path)\n",
    "\n",
    "model_for_prediciton.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore parameters and metrtics from mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should take prdicted probabilities and not predicted classes: output scores (logits) are converted into probabilities using the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definition\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import pydicom\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = model_for_prediciton\n",
    "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))  # Move model to appropriate device\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Prepare data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.data['severity'] = self.data['severity'].astype(int)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        image_path = row['image_path']\n",
    "        label = row['severity']\n",
    "        dicom_image = pydicom.dcmread(image_path)\n",
    "        image = dicom_image.pixel_array.astype(float)\n",
    "        image = (image / image.max() * 255).astype('uint8')\n",
    "        if len(image.shape) == 2:  # Grayscale\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        image_tensor = self.transform(image) if self.transform else torch.from_numpy(image).permute(2, 0, 1)\n",
    "        return image_tensor, torch.tensor(label).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting probabilities for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prbabilities and probability list\n",
    "test_dataset = MRIDataset(data=test_data, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Inference loop with probability extraction\n",
    "results = []\n",
    "probabilities_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)  # Ensure labels are also on the correct device\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)  # Calculate class probabilities\n",
    "        _, predicted_classes = torch.max(outputs, 1)\n",
    "        \n",
    "        # Append predictions and probabilities\n",
    "        results.append(predicted_classes.item())\n",
    "        probabilities_list.append(probabilities.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predicted classes and their probabilities\n",
    "for i, (pred, probs) in enumerate(zip(results, probabilities_list)):\n",
    "    print(f\"Test image {i}: Predicted class {pred}, Probabilities: {probs}\")\n",
    "\n",
    "# Additional code to plot the confusion matrix can stay as-is:\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "    true_labels.extend(labels.numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted_classes = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "print(\"True Labels Unique Values:\", np.unique(true_labels))\n",
    "print(\"Predicted Labels Unique Values:\", np.unique(predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of probabilities to a numpy array for easier manipulation\n",
    "probabilities_array = np.vstack(probabilities_list)\n",
    "\n",
    "# Ensure the length of probabilities_array matches the length of test_df\n",
    "probabilities_array = probabilities_array[:len(test_df)]\n",
    "\n",
    "# Add the probabilities to the test_df DataFrame\n",
    "test_df['Probability_Class_0'] = probabilities_array[:, 0]\n",
    "test_df['Probability_Class_1'] = probabilities_array[:, 1]\n",
    "test_df['Probability_Class_2'] = probabilities_array[:, 2]\n",
    "\n",
    "# Round the probabilities to 4 decimal places\n",
    "test_df['Probability_Class_0'] = test_df['Probability_Class_0'].round(4)\n",
    "test_df['Probability_Class_1'] = test_df['Probability_Class_1'].round(4)\n",
    "test_df['Probability_Class_2'] = test_df['Probability_Class_2'].round(4)\n",
    "# Add the predicted classes to the test_df DataFrame\n",
    "test_df['Predicted_Class'] = results\n",
    "\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    " \n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by severity and sum the Probability_Class_1\n",
    "severity_prob_sum_all_classes = test_df.groupby('severity')[['Probability_Class_0', 'Probability_Class_1', 'Probability_Class_2']].mean()\n",
    "\n",
    "# Print the result\n",
    "severity_prob_sum_all_classes\n",
    "# Use severity_prob_sum_all_classes as confusion matrix\n",
    "conf_matrix_prob_df = severity_prob_sum_all_classes \n",
    "# Create a confusion matrix from severity and predicted class\n",
    "conf_matrix_severity_pred = confusion_matrix(test_df['severity'], test_df['Predicted_Class'])\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "conf_matrix_severity_pred_df = pd.DataFrame(\n",
    "    conf_matrix_severity_pred, \n",
    "    index=[f\"Actual {i}\" for i in range(len(conf_matrix_severity_pred))], \n",
    "    columns=[f\"Predicted {i}\" for i in range(len(conf_matrix_severity_pred[0]))]\n",
    ")\n",
    "\n",
    "# Print the confusion matrix DataFrame\n",
    "print(conf_matrix_severity_pred_df)\n",
    "\n",
    "# Optionally, display it using a more formatted view (e.g., in Jupyter Notebook)\n",
    "conf_matrix_severity_pred_df.style.background_gradient(cmap='Blues')\n",
    "# Print the confusion matrix DataFrame\n",
    "print(conf_matrix_prob_df)\n",
    "\n",
    "# Optionally, display it using a more formatted view (e.g., in Jupyter Notebook)\n",
    "conf_matrix_prob_df.style.background_gradient(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_severity_pred_df.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure true_labels and predicted_labels have the same length\n",
    "min_length = min(len(true_labels), len(predicted_labels))\n",
    "true_labels = true_labels[:min_length]\n",
    "predicted_labels = predicted_labels[:min_length]\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix, \n",
    "    index=[f\"Actual {i}\" for i in range(len(conf_matrix))], \n",
    "    columns=[f\"Predicted {i}\" for i in range(len(conf_matrix[0]))]\n",
    ")\n",
    "\n",
    "# Print the confusion matrix DataFrame\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Optionally, display it using a more formatted view (e.g., in Jupyter Notebook)\n",
    "conf_matrix_df.style.background_gradient(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "# Calculate precision, recall, and accuracy using mean_probabilities_df\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "overfit_and_underfit.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
