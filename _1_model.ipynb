{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FklhSI0Gg9R"
   },
   "source": [
    "Necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    " # Import functions from the module\n",
    "import importlib\n",
    "import help_files._0_definitions \n",
    "import  help_files._1_visuals_script\n",
    "# import  help_files._01_load_data\n",
    " # Reload the module to apply the changes to the script\n",
    "importlib.reload(help_files._0_definitions)\n",
    "importlib.reload(help_files._1_visuals_script)\n",
    "# importlib.reload(help_files._01_load_data)\n",
    "import  help_files._1_visuals_script  as pauls_vs\n",
    "# Group by 'condition', 'level', and 'severity' and count occurrences\n",
    "from help_files._0_definitions import count_severity_by_condition_level \n",
    "# Define the path\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.width\", 1000)  # Set a large width to prevent line wrapping\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_definitions.py\") as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered dataframes to CSV files\n",
    "dataframes = [\"df_end\"]\n",
    "file_names = [\"df_end.csv\"]\n",
    " \n",
    "# Load the data from _01_load_data\n",
    "dataframes = [pd.read_csv(data_path_vor / file_name) for file_name in file_names]\n",
    "df_end = dataframes[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "study_id        int64\n",
       "severity      float64\n",
       "condition       int64\n",
       "level          object\n",
       "series_id     float64\n",
       "x             float64\n",
       "y             float64\n",
       "image_path     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_end.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>severity</th>\n",
       "      <th>condition</th>\n",
       "      <th>level</th>\n",
       "      <th>series_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.054714e+09</td>\n",
       "      <td>197.100569</td>\n",
       "      <td>289.457306</td>\n",
       "      <td>data/train_images_origin/4003253/702807833/7.dcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4003253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.054714e+09</td>\n",
       "      <td>197.100569</td>\n",
       "      <td>289.457306</td>\n",
       "      <td>data/train_images_origin/4003253/702807833/15.dcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4003253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.054714e+09</td>\n",
       "      <td>197.100569</td>\n",
       "      <td>289.457306</td>\n",
       "      <td>data/train_images_origin/4003253/702807833/14.dcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.054714e+09</td>\n",
       "      <td>197.100569</td>\n",
       "      <td>289.457306</td>\n",
       "      <td>data/train_images_origin/4003253/702807833/10.dcm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4646740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>3.486248e+09</td>\n",
       "      <td>235.980844</td>\n",
       "      <td>360.313610</td>\n",
       "      <td>data/train_images_origin/4646740/3666319702/13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2141458217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>4.987342e+08</td>\n",
       "      <td>177.230769</td>\n",
       "      <td>240.351648</td>\n",
       "      <td>data/train_images_origin/2141458217/869187184/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2141458217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>4.987342e+08</td>\n",
       "      <td>177.230769</td>\n",
       "      <td>240.351648</td>\n",
       "      <td>data/train_images_origin/2141458217/869187184/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2141458217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>4.987342e+08</td>\n",
       "      <td>177.230769</td>\n",
       "      <td>240.351648</td>\n",
       "      <td>data/train_images_origin/2141458217/869187184/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2141458217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>4.987342e+08</td>\n",
       "      <td>177.230769</td>\n",
       "      <td>240.351648</td>\n",
       "      <td>data/train_images_origin/2141458217/869187184/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2141458217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>4.987342e+08</td>\n",
       "      <td>177.230769</td>\n",
       "      <td>240.351648</td>\n",
       "      <td>data/train_images_origin/2141458217/869187184/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        study_id  severity  condition  level     series_id           x           y                                         image_path\n",
       "0        4003253       0.0          0  l5/s1  1.054714e+09  197.100569  289.457306   data/train_images_origin/4003253/702807833/7.dcm\n",
       "1        4003253       0.0          0  l5/s1  1.054714e+09  197.100569  289.457306  data/train_images_origin/4003253/702807833/15.dcm\n",
       "2        4003253       0.0          0  l5/s1  1.054714e+09  197.100569  289.457306  data/train_images_origin/4003253/702807833/14.dcm\n",
       "3        4003253       0.0          0  l5/s1  1.054714e+09  197.100569  289.457306  data/train_images_origin/4003253/702807833/10.dcm\n",
       "4        4646740       1.0          0  l5/s1  3.486248e+09  235.980844  360.313610  data/train_images_origin/4646740/3666319702/13...\n",
       "...          ...       ...        ...    ...           ...         ...         ...                                                ...\n",
       "2995  2141458217       2.0          0  l5/s1  4.987342e+08  177.230769  240.351648  data/train_images_origin/2141458217/869187184/...\n",
       "2996  2141458217       2.0          0  l5/s1  4.987342e+08  177.230769  240.351648  data/train_images_origin/2141458217/869187184/...\n",
       "2997  2141458217       2.0          0  l5/s1  4.987342e+08  177.230769  240.351648  data/train_images_origin/2141458217/869187184/...\n",
       "2998  2141458217       2.0          0  l5/s1  4.987342e+08  177.230769  240.351648  data/train_images_origin/2141458217/869187184/...\n",
       "2999  2141458217       2.0          0  l5/s1  4.987342e+08  177.230769  240.351648  data/train_images_origin/2141458217/869187184/...\n",
       "\n",
       "[3000 rows x 8 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>severity</th>\n",
       "      <th>condition</th>\n",
       "      <th>level</th>\n",
       "      <th>series_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>1261271580</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>5.634637e+08</td>\n",
       "      <td>154.576328</td>\n",
       "      <td>221.944761</td>\n",
       "      <td>data/train_images_origin/1261271580/813965073/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>808539750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>3.152297e+09</td>\n",
       "      <td>464.810632</td>\n",
       "      <td>624.751228</td>\n",
       "      <td>data/train_images_origin/808539750/412413083/4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>1271033295</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.040397e+09</td>\n",
       "      <td>144.600457</td>\n",
       "      <td>244.992390</td>\n",
       "      <td>data/train_images_origin/1271033295/2054979604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>178041181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>3.904103e+09</td>\n",
       "      <td>352.485414</td>\n",
       "      <td>427.421214</td>\n",
       "      <td>data/train_images_origin/178041181/2495441739/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>1746166687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.482487e+07</td>\n",
       "      <td>264.814679</td>\n",
       "      <td>368.499083</td>\n",
       "      <td>data/train_images_origin/1746166687/2015196917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>296314829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>3.788461e+09</td>\n",
       "      <td>138.819320</td>\n",
       "      <td>268.622540</td>\n",
       "      <td>data/train_images_origin/296314829/3463650248/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>1859534255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>3.931601e+09</td>\n",
       "      <td>238.748092</td>\n",
       "      <td>307.786260</td>\n",
       "      <td>data/train_images_origin/1859534255/1445343658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>1588228644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>8.639016e+08</td>\n",
       "      <td>138.507463</td>\n",
       "      <td>198.339213</td>\n",
       "      <td>data/train_images_origin/1588228644/1586605256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>109677683</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.241325e+09</td>\n",
       "      <td>308.612907</td>\n",
       "      <td>567.957571</td>\n",
       "      <td>data/train_images_origin/109677683/714837857/6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>1205664021</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>5.723447e+08</td>\n",
       "      <td>273.541384</td>\n",
       "      <td>343.880597</td>\n",
       "      <td>data/train_images_origin/1205664021/3886228128...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        study_id  severity  condition  level     series_id           x           y                                         image_path\n",
       "1801  1261271580       2.0          0  l5/s1  5.634637e+08  154.576328  221.944761  data/train_images_origin/1261271580/813965073/...\n",
       "1190   808539750       2.0          0  l5/s1  3.152297e+09  464.810632  624.751228  data/train_images_origin/808539750/412413083/4...\n",
       "1817  1271033295       2.0          0  l5/s1  1.040397e+09  144.600457  244.992390  data/train_images_origin/1271033295/2054979604...\n",
       "251    178041181       1.0          0  l5/s1  3.904103e+09  352.485414  427.421214  data/train_images_origin/178041181/2495441739/...\n",
       "2505  1746166687       1.0          0  l5/s1  1.482487e+07  264.814679  368.499083  data/train_images_origin/1746166687/2015196917...\n",
       "...          ...       ...        ...    ...           ...         ...         ...                                                ...\n",
       "416    296314829       1.0          0  l5/s1  3.788461e+09  138.819320  268.622540  data/train_images_origin/296314829/3463650248/...\n",
       "2631  1859534255       0.0          0  l5/s1  3.931601e+09  238.748092  307.786260  data/train_images_origin/1859534255/1445343658...\n",
       "2291  1588228644       0.0          0  l5/s1  8.639016e+08  138.507463  198.339213  data/train_images_origin/1588228644/1586605256...\n",
       "174    109677683       2.0          0  l5/s1  1.241325e+09  308.612907  567.957571  data/train_images_origin/109677683/714837857/6...\n",
       "1725  1205664021       2.0          0  l5/s1  5.723447e+08  273.541384  343.880597  data/train_images_origin/1205664021/3886228128...\n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sample 100 rows from random_samples_combined\n",
    "random_samples_test_check = df_end.sample(n=100, random_state=RSEED)\n",
    "random_samples_test_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ms\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation and class\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import pydicom\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Define the transform with augmentation: I already tranformed i tbfore \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    #transforms.RandomRotation(10),       # Randomly rotate the image by ±10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust color properties\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ensure severity is in integer format\n",
    "        self.data['severity'] = self.data['severity'].astype(int)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        image_path = row['image_path']\n",
    "        label = row['severity']  # Use severity for the label\n",
    "\n",
    "        dicom_image = pydicom.dcmread(image_path)\n",
    "        image = dicom_image.pixel_array.astype(float)\n",
    "        image = (image / image.max() * 255).astype('uint8')  # Normalize\n",
    "\n",
    "        # Convert the image to RGB if it is grayscale\n",
    "        if len(image.shape) == 2:  # Grayscale\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Apply transformations including augmentation\n",
    "        image_tensor = self.transform(image) if self.transform else torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        return image_tensor, torch.tensor(label).long()  # Return label as tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training model resnet50 sotp if validation and train go apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renet50\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Import numpy for setting the random seed\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42  # You can choose any integer\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)  # Set seed for numpy\n",
    "import random\n",
    "random.seed(seed)  # Set seed for random module\n",
    "\n",
    "# Create a new experiment\n",
    "experiment_name = \"Resnet50_MRI_Classification\"\n",
    "# Uncomment the line below to create a new experiment only if it doesn't already exist\n",
    "# mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = MRIDataset(data=df_end, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Load ResNet-50 and set up for classification\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = df_end['severity'].nunique()\n",
    "\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss functions and optimizer\n",
    "criterion_cel = nn.CrossEntropyLoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 30\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "train_losses_cel = []\n",
    "val_losses_cel = []\n",
    "train_losses_mse = []\n",
    "val_losses_mse = []\n",
    "\n",
    "# Early stopping parameters\n",
    "stop_threshold = 0.2  # Threshold for validation loss to diverge from training loss\n",
    "diverge_count = 0\n",
    "max_diverge_count = 3  # Number of epochs validation loss is allowed to diverge\n",
    "\n",
    "# Calculate number of layers in the model\n",
    "num_layers = len(list(model.children()))\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"learning_rate\", 0.0001)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", 4)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"num_classes\", num_classes)\n",
    "    mlflow.log_param(\"stop_threshold\", stop_threshold)\n",
    "    mlflow.log_param(\"model_architecture\", \"ResNet-50\")\n",
    "    mlflow.log_param(\"input_size\", \"224x224\")\n",
    "    mlflow.log_param(\"num_layers\", num_layers)  # Log number of layers\n",
    "    mlflow.log_param(\"activation_function\", \"ReLU\")  # Log activation function\n",
    "    mlflow.log_param(\"pretrained_weights\", \"IMAGENET1K_V1\")\n",
    "\n",
    "    # Training and validation loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss_cel_train = 0.0\n",
    "        running_loss_mse_train = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Cross Entropy Loss\n",
    "            loss_cel = criterion_cel(outputs, labels)\n",
    "            running_loss_cel_train += loss_cel.item()\n",
    "\n",
    "            # MSE Loss (if applicable)\n",
    "            mse_target = labels.float().unsqueeze(1).expand_as(outputs)  # Example target\n",
    "            loss_mse = criterion_mse(outputs, mse_target)\n",
    "            running_loss_mse_train += loss_mse.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss_cel.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate average losses\n",
    "        epoch_loss_cel_train = running_loss_cel_train / len(train_loader)\n",
    "        train_losses_cel.append(epoch_loss_cel_train)\n",
    "        epoch_loss_mse_train = running_loss_mse_train / len(train_loader)\n",
    "        train_losses_mse.append(epoch_loss_mse_train)\n",
    "\n",
    "        # Log training losses to MLflow\n",
    "        mlflow.log_metric(\"train_loss_cel\", epoch_loss_cel_train, step=epoch)\n",
    "        mlflow.log_metric(\"train_loss_mse\", epoch_loss_mse_train, step=epoch)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss_cel_val = 0.0\n",
    "        running_loss_mse_val = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Cross Entropy Loss for validation\n",
    "                loss_cel = criterion_cel(outputs, labels)\n",
    "                running_loss_cel_val += loss_cel.item()\n",
    "                \n",
    "                # MSE Loss (if applicable)\n",
    "                mse_target = labels.float().unsqueeze(1).expand_as(outputs)  # Ensure mse_target matches outputs shape\n",
    "                loss_mse = criterion_mse(outputs, mse_target)\n",
    "                running_loss_mse_val += loss_mse.item()\n",
    "\n",
    "        # Calculate validation losses\n",
    "        epoch_loss_cel_val = running_loss_cel_val / len(val_loader)\n",
    "        val_losses_cel.append(epoch_loss_cel_val)\n",
    "        epoch_loss_mse_val = running_loss_mse_val / len(val_loader)\n",
    "        val_losses_mse.append(epoch_loss_mse_val)\n",
    "\n",
    "        # Log validation losses to MLflow\n",
    "        mlflow.log_metric(\"val_loss_cel\", epoch_loss_cel_val, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss_mse\", epoch_loss_mse_val, step=epoch)\n",
    "\n",
    "        # Early stopping check\n",
    "        if epoch_loss_cel_val > epoch_loss_cel_train * (1 + stop_threshold):\n",
    "            diverge_count += 1\n",
    "            if diverge_count >= max_diverge_count:\n",
    "                print(f\"Early stopping at epoch {epoch+1} due to validation loss diverging.\")\n",
    "                break\n",
    "        else:\n",
    "            diverge_count = 0  # Reset count if validation loss improves\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Cross Entropy Loss: {epoch_loss_cel_train:.4f}, '\n",
    "              f'Validation Cross Entropy Loss: {epoch_loss_cel_val:.4f}')\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    # Plot and log the loss curves as artifacts\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_cel, label='Train Cross Entropy Loss')\n",
    "    plt.plot(val_losses_cel, label='Validation Cross Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Cross Entropy Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"cross_entropy_loss.png\")\n",
    "    mlflow.log_artifact(\"cross_entropy_loss.png\")\n",
    "\n",
    "    # Plot MSE Loss if applicable\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_mse, label='Train MSE Loss')\n",
    "    plt.plot(val_losses_mse, label='Validation MSE Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"mse_loss.png\")\n",
    "    mlflow.log_artifact(\"mse_loss.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore parameters and metrtics from mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.stop_threshold params.optimizer params.num_epochs params.learning_rate params.batch_size params.num_classes params.num_layers params.pretrained_weights params.model_architecture params.activation_function params.input_size\n",
      "                  0.2             Adam                10               0.0001                 4                  3              None                      None                      None                       None              None\n",
      "                  0.2             Adam                30               0.0001                 4                  3                10             IMAGENET1K_V1                 ResNet-50                       ReLU           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3                10             IMAGENET1K_V1                 ResNet-50                       ReLU           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3                10             IMAGENET1K_V1                 ResNet-50                       ReLU           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3                10             IMAGENET1K_V1                 ResNet-50                       ReLU           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3                10             IMAGENET1K_V1                 ResNet-50                       ReLU           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3                10             IMAGENET1K_V1                 ResNet-50                       ReLU           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3                10             IMAGENET1K_V1                 ResNet-50                       ReLU           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3              None                      None                 ResNet-50                       None           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3                10             IMAGENET1K_V1                 ResNet-50                       None           224x224\n",
      "                  0.2             Adam                10               0.0001                 4                  3              None                      None                      None                       None              None\n",
      "                  0.2             Adam                10               0.0001                 4                  3              None                      None                      None                       None              None\n",
      "                  0.2             Adam                10               0.0001                 4                  3              None                      None                      None                       None              None\n",
      " metrics.val_loss_mse  metrics.train_loss_mse  metrics.train_loss_cel  metrics.val_loss_cel\n",
      "                  NaN                     NaN                     NaN                   NaN\n",
      "             7.152682                6.985822                0.316386              0.655457\n",
      "                  NaN                     NaN                     NaN                   NaN\n",
      "             2.633196                2.289454                0.839326              1.037692\n",
      "                  NaN                     NaN                     NaN                   NaN\n",
      "             2.573524                1.622374                0.641722              1.202776\n",
      "             0.719049                3.137755                0.366277              1.510131\n",
      "             1.279556                1.884438                0.495031              0.776052\n",
      "                  NaN                     NaN                     NaN                   NaN\n",
      "             2.252977                1.944706                0.677454              1.589684\n",
      "             2.269442                2.881829                0.618881              1.217518\n",
      "             2.847658                2.885441                0.493284              1.437710\n",
      "             3.318389                1.438336                0.737927              1.502563\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP1\\AppData\\Local\\Temp\\ipykernel_7272\\3780372766.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)  # Convert labels to tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image 0: Predicted class 2\n",
      "Test image 1: Predicted class 2\n",
      "Test image 2: Predicted class 2\n",
      "Test image 3: Predicted class 2\n",
      "Test image 4: Predicted class 0\n",
      "Test image 5: Predicted class 2\n",
      "Test image 6: Predicted class 1\n",
      "Test image 7: Predicted class 2\n",
      "Test image 8: Predicted class 2\n",
      "Test image 9: Predicted class 2\n",
      "Test image 10: Predicted class 0\n",
      "Test image 11: Predicted class 2\n",
      "Test image 12: Predicted class 1\n",
      "Test image 13: Predicted class 0\n",
      "Test image 14: Predicted class 2\n",
      "Test image 15: Predicted class 2\n",
      "Test image 16: Predicted class 2\n",
      "Test image 17: Predicted class 1\n",
      "Test image 18: Predicted class 2\n",
      "Test image 19: Predicted class 2\n",
      "Test image 20: Predicted class 2\n",
      "Test image 21: Predicted class 2\n",
      "Test image 22: Predicted class 2\n",
      "Test image 23: Predicted class 2\n",
      "Test image 24: Predicted class 1\n",
      "Test image 25: Predicted class 1\n",
      "Test image 26: Predicted class 1\n",
      "Test image 27: Predicted class 2\n",
      "Test image 28: Predicted class 0\n",
      "Test image 29: Predicted class 2\n",
      "Test image 30: Predicted class 2\n",
      "Test image 31: Predicted class 1\n",
      "Test image 32: Predicted class 2\n",
      "Test image 33: Predicted class 2\n",
      "Test image 34: Predicted class 2\n",
      "Test image 35: Predicted class 0\n",
      "Test image 36: Predicted class 2\n",
      "Test image 37: Predicted class 2\n",
      "Test image 38: Predicted class 2\n",
      "Test image 39: Predicted class 2\n",
      "Test image 40: Predicted class 2\n",
      "Test image 41: Predicted class 2\n",
      "Test image 42: Predicted class 2\n",
      "Test image 43: Predicted class 2\n",
      "Test image 44: Predicted class 2\n",
      "Test image 45: Predicted class 0\n",
      "Test image 46: Predicted class 2\n",
      "Test image 47: Predicted class 2\n",
      "Test image 48: Predicted class 1\n",
      "Test image 49: Predicted class 1\n",
      "Test image 50: Predicted class 2\n",
      "Test image 51: Predicted class 2\n",
      "Test image 52: Predicted class 2\n",
      "Test image 53: Predicted class 2\n",
      "Test image 54: Predicted class 2\n",
      "Test image 55: Predicted class 1\n",
      "Test image 56: Predicted class 2\n",
      "Test image 57: Predicted class 0\n",
      "Test image 58: Predicted class 2\n",
      "Test image 59: Predicted class 1\n",
      "Test image 60: Predicted class 1\n",
      "Test image 61: Predicted class 2\n",
      "Test image 62: Predicted class 1\n",
      "Test image 63: Predicted class 2\n",
      "Test image 64: Predicted class 0\n",
      "Test image 65: Predicted class 2\n",
      "Test image 66: Predicted class 0\n",
      "Test image 67: Predicted class 2\n",
      "Test image 68: Predicted class 1\n",
      "Test image 69: Predicted class 0\n",
      "Test image 70: Predicted class 2\n",
      "Test image 71: Predicted class 2\n",
      "Test image 72: Predicted class 2\n",
      "Test image 73: Predicted class 1\n",
      "Test image 74: Predicted class 2\n",
      "Test image 75: Predicted class 2\n",
      "Test image 76: Predicted class 2\n",
      "Test image 77: Predicted class 2\n",
      "Test image 78: Predicted class 2\n",
      "Test image 79: Predicted class 1\n",
      "Test image 80: Predicted class 2\n",
      "Test image 81: Predicted class 2\n",
      "Test image 82: Predicted class 2\n",
      "Test image 83: Predicted class 2\n",
      "Test image 84: Predicted class 2\n",
      "Test image 85: Predicted class 1\n",
      "Test image 86: Predicted class 0\n",
      "Test image 87: Predicted class 2\n",
      "Test image 88: Predicted class 2\n",
      "Test image 89: Predicted class 2\n",
      "Test image 90: Predicted class 2\n",
      "Test image 91: Predicted class 2\n",
      "Test image 92: Predicted class 2\n",
      "Test image 93: Predicted class 2\n",
      "Test image 94: Predicted class 2\n",
      "Test image 95: Predicted class 2\n",
      "Test image 96: Predicted class 2\n",
      "Test image 97: Predicted class 2\n",
      "Test image 98: Predicted class 1\n",
      "Test image 99: Predicted class 1\n",
      "Test image 100: Predicted class 2\n",
      "Test image 101: Predicted class 2\n",
      "Test image 102: Predicted class 2\n",
      "Test image 103: Predicted class 0\n",
      "Test image 104: Predicted class 0\n",
      "Test image 105: Predicted class 1\n",
      "Test image 106: Predicted class 2\n",
      "Test image 107: Predicted class 2\n",
      "Test image 108: Predicted class 2\n",
      "Test image 109: Predicted class 2\n",
      "Test image 110: Predicted class 1\n",
      "Test image 111: Predicted class 2\n",
      "Test image 112: Predicted class 2\n",
      "Test image 113: Predicted class 2\n",
      "Test image 114: Predicted class 2\n",
      "Test image 115: Predicted class 2\n",
      "Test image 116: Predicted class 1\n",
      "Test image 117: Predicted class 1\n",
      "Test image 118: Predicted class 2\n",
      "Test image 119: Predicted class 2\n",
      "Test image 120: Predicted class 2\n",
      "Test image 121: Predicted class 2\n",
      "Test image 122: Predicted class 2\n",
      "Test image 123: Predicted class 1\n",
      "Test image 124: Predicted class 2\n",
      "Test image 125: Predicted class 2\n",
      "Test image 126: Predicted class 2\n",
      "Test image 127: Predicted class 2\n",
      "Test image 128: Predicted class 2\n",
      "Test image 129: Predicted class 2\n",
      "Test image 130: Predicted class 2\n",
      "Test image 131: Predicted class 0\n",
      "Test image 132: Predicted class 1\n",
      "Test image 133: Predicted class 2\n",
      "Test image 134: Predicted class 2\n",
      "Test image 135: Predicted class 2\n",
      "Test image 136: Predicted class 2\n",
      "Test image 137: Predicted class 0\n",
      "Test image 138: Predicted class 1\n",
      "Test image 139: Predicted class 2\n",
      "Test image 140: Predicted class 2\n",
      "Test image 141: Predicted class 1\n",
      "Test image 142: Predicted class 2\n",
      "Test image 143: Predicted class 2\n",
      "Test image 144: Predicted class 2\n",
      "Test image 145: Predicted class 1\n",
      "Test image 146: Predicted class 2\n",
      "Test image 147: Predicted class 0\n",
      "Test image 148: Predicted class 1\n",
      "Test image 149: Predicted class 2\n",
      "Test image 150: Predicted class 2\n",
      "Test image 151: Predicted class 2\n",
      "Test image 152: Predicted class 2\n",
      "Test image 153: Predicted class 2\n",
      "Test image 154: Predicted class 1\n",
      "Test image 155: Predicted class 0\n",
      "Test image 156: Predicted class 2\n",
      "Test image 157: Predicted class 0\n",
      "Test image 158: Predicted class 2\n",
      "Test image 159: Predicted class 2\n",
      "Test image 160: Predicted class 1\n",
      "Test image 161: Predicted class 2\n",
      "Test image 162: Predicted class 1\n",
      "Test image 163: Predicted class 1\n",
      "Test image 164: Predicted class 0\n",
      "Test image 165: Predicted class 2\n",
      "Test image 166: Predicted class 2\n",
      "Test image 167: Predicted class 1\n",
      "Test image 168: Predicted class 1\n",
      "Test image 169: Predicted class 2\n",
      "Test image 170: Predicted class 2\n",
      "Test image 171: Predicted class 2\n",
      "Test image 172: Predicted class 2\n",
      "Test image 173: Predicted class 2\n",
      "Test image 174: Predicted class 1\n",
      "Test image 175: Predicted class 2\n",
      "Test image 176: Predicted class 2\n",
      "Test image 177: Predicted class 2\n",
      "Test image 178: Predicted class 2\n",
      "Test image 179: Predicted class 2\n",
      "Test image 180: Predicted class 2\n",
      "Test image 181: Predicted class 2\n",
      "Test image 182: Predicted class 2\n",
      "Test image 183: Predicted class 2\n",
      "Test image 184: Predicted class 2\n",
      "Test image 185: Predicted class 2\n",
      "Test image 186: Predicted class 2\n",
      "Test image 187: Predicted class 0\n",
      "Test image 188: Predicted class 1\n",
      "Test image 189: Predicted class 2\n",
      "Test image 190: Predicted class 0\n",
      "Test image 191: Predicted class 2\n",
      "Test image 192: Predicted class 1\n",
      "Test image 193: Predicted class 2\n",
      "Test image 194: Predicted class 2\n",
      "Test image 195: Predicted class 1\n",
      "Test image 196: Predicted class 1\n",
      "Test image 197: Predicted class 0\n",
      "Test image 198: Predicted class 2\n",
      "Test image 199: Predicted class 2\n",
      "Test image 200: Predicted class 2\n",
      "Test image 201: Predicted class 2\n",
      "Test image 202: Predicted class 1\n",
      "Test image 203: Predicted class 2\n",
      "Test image 204: Predicted class 2\n",
      "Test image 205: Predicted class 1\n",
      "Test image 206: Predicted class 2\n",
      "Test image 207: Predicted class 1\n",
      "Test image 208: Predicted class 0\n",
      "Test image 209: Predicted class 2\n",
      "Test image 210: Predicted class 2\n",
      "Test image 211: Predicted class 1\n",
      "Test image 212: Predicted class 1\n",
      "Test image 213: Predicted class 2\n",
      "Test image 214: Predicted class 2\n",
      "Test image 215: Predicted class 2\n",
      "Test image 216: Predicted class 2\n",
      "Test image 217: Predicted class 2\n",
      "Test image 218: Predicted class 2\n",
      "Test image 219: Predicted class 1\n",
      "Test image 220: Predicted class 2\n",
      "Test image 221: Predicted class 1\n",
      "Test image 222: Predicted class 2\n",
      "Test image 223: Predicted class 2\n",
      "Test image 224: Predicted class 2\n",
      "Test image 225: Predicted class 0\n",
      "Test image 226: Predicted class 2\n",
      "Test image 227: Predicted class 0\n",
      "Test image 228: Predicted class 2\n",
      "Test image 229: Predicted class 2\n",
      "Test image 230: Predicted class 2\n",
      "Test image 231: Predicted class 1\n",
      "Test image 232: Predicted class 1\n",
      "Test image 233: Predicted class 1\n",
      "Test image 234: Predicted class 2\n",
      "Test image 235: Predicted class 2\n",
      "Test image 236: Predicted class 0\n",
      "Test image 237: Predicted class 2\n",
      "Test image 238: Predicted class 2\n",
      "Test image 239: Predicted class 2\n",
      "Test image 240: Predicted class 2\n",
      "Test image 241: Predicted class 2\n",
      "Test image 242: Predicted class 0\n",
      "Test image 243: Predicted class 1\n",
      "Test image 244: Predicted class 2\n",
      "Test image 245: Predicted class 2\n",
      "Test image 246: Predicted class 2\n",
      "Test image 247: Predicted class 0\n",
      "Test image 248: Predicted class 2\n",
      "Test image 249: Predicted class 2\n",
      "Test image 250: Predicted class 2\n",
      "Test image 251: Predicted class 2\n",
      "Test image 252: Predicted class 2\n",
      "Test image 253: Predicted class 2\n",
      "Test image 254: Predicted class 1\n",
      "Test image 255: Predicted class 0\n",
      "Test image 256: Predicted class 2\n",
      "Test image 257: Predicted class 2\n",
      "Test image 258: Predicted class 1\n",
      "Test image 259: Predicted class 2\n",
      "Test image 260: Predicted class 2\n",
      "Test image 261: Predicted class 2\n",
      "Test image 262: Predicted class 1\n",
      "Test image 263: Predicted class 2\n",
      "Test image 264: Predicted class 2\n",
      "Test image 265: Predicted class 2\n",
      "Test image 266: Predicted class 1\n",
      "Test image 267: Predicted class 1\n",
      "Test image 268: Predicted class 1\n",
      "Test image 269: Predicted class 0\n",
      "Test image 270: Predicted class 1\n",
      "Test image 271: Predicted class 1\n",
      "Test image 272: Predicted class 2\n",
      "Test image 273: Predicted class 1\n",
      "Test image 274: Predicted class 2\n",
      "Test image 275: Predicted class 2\n",
      "Test image 276: Predicted class 1\n",
      "Test image 277: Predicted class 2\n",
      "Test image 278: Predicted class 2\n",
      "Test image 279: Predicted class 2\n",
      "Test image 280: Predicted class 2\n",
      "Test image 281: Predicted class 1\n",
      "Test image 282: Predicted class 2\n",
      "Test image 283: Predicted class 1\n",
      "Test image 284: Predicted class 2\n",
      "Test image 285: Predicted class 2\n",
      "Test image 286: Predicted class 2\n",
      "Test image 287: Predicted class 2\n",
      "Test image 288: Predicted class 2\n",
      "Test image 289: Predicted class 2\n",
      "Test image 290: Predicted class 2\n",
      "Test image 291: Predicted class 2\n",
      "Test image 292: Predicted class 2\n",
      "Test image 293: Predicted class 0\n",
      "Test image 294: Predicted class 2\n",
      "Test image 295: Predicted class 2\n",
      "Test image 296: Predicted class 1\n",
      "Test image 297: Predicted class 1\n",
      "Test image 298: Predicted class 2\n",
      "Test image 299: Predicted class 2\n",
      "True Labels Unique Values: [0 1 2]\n",
      "Predicted Labels Unique Values: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "test_df =df_end.sample(n=300, random_state=RSEED)\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Set the experiment name (optional, helps in identifying runs)\n",
    "experiment_name = \"Resnet50_MRI_Classification\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Specify the run ID of the logged model\n",
    "run_id = \"f390913c59d642329c86d0f52b943062\"  # Replace with your actual run ID\n",
    "\n",
    "# Create the model URI\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "# Load the model\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "# Now you can use the model for inference or evaluation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "# Get all runs for the experiment\n",
    "runs = mlflow.search_runs(experiment_names=[experiment_name])\n",
    "\n",
    "print(runs.filter(like='params.').to_string(index=False))\n",
    "# Display all metrics\n",
    "metrics_columns = [col for col in runs.columns if col.startswith('metrics.')]\n",
    "print(runs[metrics_columns].to_string(index=False))\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import pydicom\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Define the transform with augmentation: I already tranformed i tbfore \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    #transforms.RandomRotation(10),       # Randomly rotate the image by ±10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust color properties\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ensure severity is in integer format\n",
    "        self.data['severity'] = self.data['severity'].astype(int)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        image_path = row['image_path']\n",
    "        label = row['severity']  # Use severity for the label\n",
    "\n",
    "        dicom_image = pydicom.dcmread(image_path)\n",
    "        image = dicom_image.pixel_array.astype(float)\n",
    "        image = (image / image.max() * 255).astype('uint8')  # Normalize\n",
    "\n",
    "        # Convert the image to RGB if it is grayscale\n",
    "        if len(image.shape) == 2:  # Grayscale\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Apply transformations including augmentation\n",
    "        image_tensor = self.transform(image) if self.transform else torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        return image_tensor, torch.tensor(label).long()  # Return label as tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Replace 'df_test' with your actual DataFrame containing test data\n",
    "test_dataset = MRIDataset(data=test_df, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Adjust batch size as needed\n",
    "test_dataset\n",
    "test_loader\n",
    "\n",
    "run_id = \"f390913c59d642329c86d0f52b943062\"  # Replace with your actual run ID\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = torch.tensor(labels)  # Convert labels to tensor\n",
    "        outputs = model(images)\n",
    "        _, predicted_classes = torch.max(outputs, 1)\n",
    "        \n",
    "        # Append predictions with corresponding image paths or indices\n",
    "        results.append(predicted_classes.item())\n",
    "\n",
    "# Print or save predictions as needed\n",
    "for i, pred in enumerate(results):\n",
    "    print(f\"Test image {i}: Predicted class {pred}\")\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store true labels and predictions\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Collect true labels and predicted labels\n",
    "for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Store the true labels\n",
    "    true_labels.extend(labels.numpy())  # Convert to numpy and extend the list\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted_classes = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted_classes.cpu().numpy())  # Move to CPU and convert to list\n",
    "\n",
    "# Convert to numpy arrays for confusion matrix\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# Check the unique values in the true and predicted labels\n",
    "print(\"True Labels Unique Values:\", np.unique(true_labels))\n",
    "print(\"Predicted Labels Unique Values:\", np.unique(predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the confusion matrix, explicitly specifying labels\n",
    "cm = confusion_matrix(true_labels, predicted_labels, labels=[0, 1, 2])\n",
    "\n",
    "# Display the confusion matrix with fixed display labels\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3733\n",
      "Precision: 0.3858\n",
      "Recall: 0.3733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_labels\n",
      "0    1.510870\n",
      "1    1.611650\n",
      "2    1.609524\n",
      "Name: predicted_labels, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with true labels and predicted labels\n",
    "results_df = pd.DataFrame({\n",
    "    'true_labels': true_labels,\n",
    "    'predicted_labels': predicted_labels\n",
    "})\n",
    "\n",
    "# Group by true labels (severity) and calculate the mean of predicted labels for each category\n",
    "predicted_mean_by_severity = results_df.groupby('true_labels')['predicted_labels'].mean()\n",
    "\n",
    "print(predicted_mean_by_severity)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "overfit_and_underfit.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
