{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FklhSI0Gg9R"
   },
   "source": [
    "Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# %pip install pydicom\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    " # Import functions from the module\n",
    "import importlib\n",
    "import help_files._0_definitions \n",
    "import  help_files._1_visuals_script\n",
    "# import  help_files._01_load_data\n",
    " # Reload the module to apply the changes to the script\n",
    "importlib.reload(help_files._0_definitions)\n",
    "importlib.reload(help_files._1_visuals_script)\n",
    "# importlib.reload(help_files._01_load_data)\n",
    "import  help_files._1_visuals_script  as pauls_vs\n",
    "# Group by 'condition', 'level', and 'severity' and count occurrences\n",
    "from help_files._0_definitions import count_severity_by_condition_level \n",
    "# Define the path\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.width\", 1000)  # Set a large width to prevent line wrapping\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_definitions.py\") as file:\n",
    "    exec(file.read())\n",
    "### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_run_definitions.py\") as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file names and DataFrame variable names\n",
    "file_names = [\"train_df_2.csv\"]\n",
    "# Load the data from the CSV files\n",
    "dataframes = [pd.read_csv(data_path_vor / file_name) for file_name in file_names]\n",
    "# Load train_df as a DataFrame\n",
    "train_df = dataframes[0]\n",
    "print(\"DataFrames have been loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* class MRIDataset(Dataset): and trascformation is run in _0_run_defintions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # end sample or small sample    \n",
    "if whole_data_set == True:\n",
    "    print(\"Using the whole data set\")\n",
    "else:\n",
    "    train_df = train_df.sample(n=20, random_state=RSEED)\n",
    "    display(Markdown('<span style=\"color:red\"> this is a small sample : 48692</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set experiment name and run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name (optional, helps in identifying runs)\n",
    "experiment_name = \"Resnet50_2_cat_basic\"\n",
    "# Specify the run ID of the logged model\n",
    "run_id = \"0eff88ff672544bdb0221d0c6415595d\"  # Replace with your actual run ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore parameters and metrtics from mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this only for checking previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# %pip install torchsummary\n",
    "from torchsummary import summary\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Set the experiment name (optional, helps in identifying runs)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Specify the run ID of the logged model\n",
    "run_id = run_id  # Replace with your actual run ID\n",
    "# Create the model URI\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "# Load the model\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "# Now you can use the model for inference or evaluation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "# Get all runs for the experiment\n",
    "runs = mlflow.search_runs(experiment_names=[experiment_name])\n",
    "\n",
    "print(runs.filter(like='params.').to_string(index=False))\n",
    "# Display all metrics\n",
    "metrics_columns = [col for col in runs.columns if col.startswith('metrics.')]\n",
    "print(runs[metrics_columns].to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model summary\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Each Column of Model\n",
    "\n",
    "### Layer (type):\n",
    "This column lists the name and type of each layer in the model. For example, `Conv2d` represents a 2D convolutional layer, `BatchNorm2d` represents a 2D batch normalization layer, `ReLU` represents a rectified linear unit activation function, and so on.\n",
    "\n",
    "### Output Shape:\n",
    "This column shows the shape of the output tensor from each layer. The shape is represented as `[-1, channels, height, width]`, where `-1` indicates a variable batch size, `channels` is the number of feature maps, and `height` and `width` are the spatial dimensions of the feature maps.\n",
    "\n",
    "### Param #:\n",
    "This column indicates the number of trainable parameters in each layer. Parameters include weights and biases that are learned during training.\n",
    "\n",
    "## Detailed Breakdown of Some Layers\n",
    "\n",
    "### Conv2d-1:\n",
    "- **Type:** Conv2d\n",
    "- **Output Shape:** `[-1, 64, 112, 112]`\n",
    "- **Param #:** 9,408\n",
    "- **Description:** This is the first convolutional layer with 64 filters, each of size 7x7, applied to the input image. The output feature maps have a spatial resolution of 112x112.\n",
    "\n",
    "### BatchNorm2d-2:\n",
    "- **Type:** BatchNorm2d\n",
    "- **Output Shape:** `[-1, 64, 112, 112]`\n",
    "- **Param #:** 128\n",
    "- **Description:** This layer normalizes the output of the previous convolutional layer to improve training stability and performance.\n",
    "\n",
    "### ReLU-3:\n",
    "- **Type:** ReLU\n",
    "- **Output Shape:** `[-1, 64, 112, 112]`\n",
    "- **Param #:** 0\n",
    "- **Description:** This layer applies the ReLU activation function, which introduces non-linearity to the model.\n",
    "\n",
    "### MaxPool2d-4:\n",
    "- **Type:** MaxPool2d\n",
    "- **Output Shape:** `[-1, 64, 56, 56]`\n",
    "- **Param #:** 0\n",
    "- **Description:** This layer performs max pooling, reducing the spatial dimensions of the feature maps to 56x56.\n",
    "\n",
    "### Conv2d-5:\n",
    "- **Type:** Conv2d\n",
    "- **Output Shape:** `[-1, 64, 56, 56]`\n",
    "- **Param #:** 4,096\n",
    "- **Description:** This is another convolutional layer with 64 filters, each of size 3x3, applied to the feature maps from the previous layer.\n",
    "\n",
    "### BatchNorm2d-6:\n",
    "- **Type:** BatchNorm2d\n",
    "- **Output Shape:** `[-1, 64, 56, 56]`\n",
    "- **Param #:** 128\n",
    "- **Description:** This layer normalizes the output of the previous convolutional layer.\n",
    "\n",
    "### ReLU-7:\n",
    "- **Type:** ReLU\n",
    "- **Output Shape:** `[-1, 64, 56, 56]`\n",
    "- **Param #:** 0\n",
    "- **Description:** This layer applies the ReLU activation function.\n",
    "\n",
    "### Bottleneck-16:\n",
    "- **Type:** Bottleneck\n",
    "- **Output Shape:** `[-1, 256, 56, 56]`\n",
    "- **Param #:** 0\n",
    "- **Description:** This is a bottleneck block, which is a key component of ResNet architectures. It consists of multiple convolutional layers and skip connections.\n",
    "\n",
    "#### Bottleneck-172\n",
    "- **Type:** Bottleneck\n",
    "- **Output Shape:** `[-1, 2048, 7, 7]`\n",
    "- **Param #:** 0\n",
    "- **Description:** This is a bottleneck block, which is a key component of ResNet architectures. It consists of multiple convolutional layers and skip connections.\n",
    "\n",
    "#### AdaptiveAvgPool2d-173\n",
    "- **Type:** AdaptiveAvgPool2d\n",
    "- **Output Shape:** `[-1, 2048, 1, 1]`\n",
    "- **Param #:** 0\n",
    "- **Description:** This layer performs adaptive average pooling, reducing the spatial dimensions of the feature maps to 1x1 while maintaining the number of channels.\n",
    "\n",
    "#### Linear-174\n",
    "- **Type:** Linear\n",
    "- **Output Shape:** `[-1, 3]`\n",
    "- **Param #:** 6,147\n",
    "- **Description:** This is a fully connected (dense) layer with 3 output units, typically used for classification tasks. The number of parameters includes the weights and biases.\n",
    "\n",
    "## Summary of the Model\n",
    "\n",
    "- **Total Parameters:** 23,514,179\n",
    "- **Trainable Parameters:** 23,514,179\n",
    "- **Non-trainable Parameters:** 0\n",
    "- **Input Size (MB):** 0.57\n",
    "- **Forward/Backward Pass Size (MB):** 286.55\n",
    "- **Params Size (MB):** 89.70\n",
    "- **Estimated Total Size (MB):** 376.82\n",
    "\n",
    "## Explanation of the Summary\n",
    "\n",
    "- **Total Parameters:** The total number of parameters in the model, including both trainable and non-trainable parameters.\n",
    "- **Trainable Parameters:** The number of parameters that are updated during training.\n",
    "- **Non-trainable Parameters:** The number of parameters that are not updated during training (e.g., fixed weights).\n",
    "- **Input Size (MB):** The memory size of the input data.\n",
    "- **Forward/Backward Pass Size (MB):** The memory size required for the forward and backward passes during training.\n",
    "- **Params Size (MB):** The memory size of the model parameters.\n",
    "- **Estimated Total Size (MB):** The estimated total memory size required for the model.\n",
    "\n",
    "This detailed breakdown helps in understanding the architecture and complexity of the ResNet-like model, including the number of layers, their types, output shapes, and the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming pretrained model: freezing layers and i# Set the model to evaluation mode\n",
    "model.eval()ntroduce addtional ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ttrasformaiton and dataloader (do not change over models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation and class\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# %pip install torch torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import pydicom\n",
    "# %pip install opencv-python\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_run_definitions.py\") as file:\n",
    "    exec(file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import parameers (weights) from previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42  # You can choose any integer\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Set up the experiment in MLflow\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Specify the run ID and load the model from MLflow\n",
    "run_id = run_id # Replace with your actual run ID\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "# Load the model\n",
    "m_1 = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "# Verify the model is loaded correctly\n",
    "print(\"Loaded Model:\")\n",
    "print(m_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing layer and defining new one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "m_1.to(device)\n",
    "\n",
    "# Freeze all layers of the loaded model\n",
    "for param in m_1.parameters():\n",
    "    param.requires_grad = False  # Freeze all parameters\n",
    "\n",
    "# Modify the fully connected (fc) layer to match new requirements\n",
    "m_1.fc = nn.Sequential(\n",
    "    nn.Linear(m_1.fc.in_features, 512),  # Intermediate fully connected layer to last RsNet50 layer\n",
    "    nn.ReLU(),                           # Activation function\n",
    "    nn.Dropout(0.5),                     # 50 percent are  dropout\n",
    "    nn.Linear(512, train_df['severity'].nunique())  # Final layer for classification\n",
    ")\n",
    "\n",
    "# Move the modified model to the appropriate device (GPU or CPU) again, in case of changes\n",
    "m_1.to(device)\n",
    "\n",
    "# Set up training dataset and DataLoaders  \n",
    "dataset = MRIDataset(data=train_df, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* some settings: train validation,  Early stopping parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models\n",
    "# %pip install keras\n",
    "# %pip install tensorflow\n",
    "# from tensorflow.keras.callbacks import EarlyStopping  # Import EarlyStopping\n",
    "import numpy as np  # Import numpy for setting the random seed\n",
    "\n",
    "# Create the dataset\n",
    "dataset = MRIDataset(data=train_data, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Define loss functions and optimizer\n",
    "criterion_cel = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(m_1.parameters(), lr=0.0001)\n",
    "num_epochs = 30\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "train_losses_cel = []\n",
    "val_losses_cel = []\n",
    "\n",
    "# Early stopping parameters\n",
    "stop_threshold = 0.2  # Threshold for validation loss to diverge from training loss\n",
    "diverge_count = 0\n",
    "max_diverge_count = 3  # Number of epochs validation loss is allowed to diverge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* setting mlflow: end running new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(nested=True):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"learning_rate\", 0.0001)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", 4)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"model_architecture\", \"Modified ResNet-50\")\n",
    "    mlflow.log_param(\"input_size\", \"224x224\")\n",
    "    mlflow.log_param(\"num_classes\", train_df['severity'].nunique())\n",
    "\n",
    "    # Training and validation loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        m_1.train()\n",
    "        running_loss_cel_train = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = m_1(images)\n",
    "            \n",
    "            # Cross Entropy Loss\n",
    "            loss_cel = criterion_cel(outputs, labels)\n",
    "            running_loss_cel_train += loss_cel.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss_cel.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate average losses\n",
    "        epoch_loss_cel_train = running_loss_cel_train / len(train_loader)\n",
    "        train_losses_cel.append(epoch_loss_cel_train)\n",
    "\n",
    "        # Log training losses to MLflow\n",
    "        mlflow.log_metric(\"train_loss_cel\", epoch_loss_cel_train, step=epoch)\n",
    "\n",
    "        # Validation phase\n",
    "        m_1.eval()\n",
    "        running_loss_cel_val = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = m_1(images)\n",
    "                \n",
    "                # Cross Entropy Loss for validation\n",
    "                loss_cel = criterion_cel(outputs, labels)\n",
    "                running_loss_cel_val += loss_cel.item()\n",
    "\n",
    "        # Calculate validation losses\n",
    "        epoch_loss_cel_val = running_loss_cel_val / len(val_loader)\n",
    "        val_losses_cel.append(epoch_loss_cel_val)\n",
    "\n",
    "\n",
    "        # Log validation losses to MLflow\n",
    "        mlflow.log_metric(\"val_loss_cel\", epoch_loss_cel_val, step=epoch)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience, patience_counter = 5, 0  # for example, wait for 5 epochs with no improvement\n",
    "\n",
    "        # pausls Set parameters for both early stopping methods sonst nimm die alte verison\n",
    "        stop_threshold = 0.1           # Threshold for divergence\n",
    "        max_diverge_count = 3          # Max number of epochs with diverging validation loss\n",
    "        patience = 5                   # Number of epochs for validation loss plateau\n",
    "        best_val_loss = float('inf')   # Initialize best validation loss\n",
    "        diverge_count = 0              # Counter for divergence-based stopping\n",
    "        patience_counter = 0           # Counter for plateau-based stopping     \n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training and validation steps\n",
    "            # Assume epoch_loss_cel_train and epoch_loss_cel_val are calculated after each epoch        \n",
    "\n",
    "            # Check divergence-based stopping\n",
    "            if epoch_loss_cel_val > epoch_loss_cel_train * (1 + stop_threshold):\n",
    "                diverge_count += 1\n",
    "                if diverge_count >= max_diverge_count:\n",
    "                    print(f\"Early stopping at epoch {epoch+1} due to validation loss diverging.\")\n",
    "                    break\n",
    "            else:\n",
    "                diverge_count = 0  # Reset diverge count if validation loss is not diverging        \n",
    "\n",
    "            # Check plateau-based stopping\n",
    "        if epoch_loss_cel_val < best_val_loss:\n",
    "                best_val_loss = epoch_loss_cel_val\n",
    "                patience_counter = 0  # Reset plateau counter if validation loss improves\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1} due to lack of validation loss improvement.\")\n",
    "                    break\n",
    "            # Print epoch results\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Cross Entropy Loss: {epoch_loss_cel_train:.4f}, '\n",
    "              f'Validation Cross Entropy Loss: {epoch_loss_cel_val:.4f}')    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.pytorch.log_model(m_1, \"modified_model\")\n",
    "\n",
    "    # passing to mlflow\n",
    "   # Plot and log the loss curves as artifacts a\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses_cel, label='Train Cross Entropy Loss')\n",
    "    plt.plot(val_losses_cel, label='Validation Cross Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Cross Entropy Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"cross_entropy_loss.png\")\n",
    "    mlflow.log_artifact(\"cross_entropy_loss.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and log the loss curves as artifacts\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses_cel, label='Train Cross Entropy Loss')\n",
    "plt.plot(val_losses_cel, label='Validation Cross Entropy Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Cross Entropy Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"cross_entropy_loss.png\")\n",
    "mlflow.log_artifact(\"cross_entropy_loss.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many layers are frozen\n",
    "frozen_layers_count = sum(1 for param in m_1.parameters() if not param.requires_grad)\n",
    "\n",
    "# Get total number of layers (parameters)\n",
    "total_layers_count = sum(1 for _ in m_1.parameters())\n",
    "\n",
    "print(f\"Frozen layers: {frozen_layers_count}/{total_layers_count} ({(frozen_layers_count / total_layers_count) * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the status of each layer\n",
    "for name, param in m_1.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"Layer '{name}' is frozen.\")\n",
    "    else:\n",
    "        print(f\"Layer '{name}' is trainable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "overfit_and_underfit.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
