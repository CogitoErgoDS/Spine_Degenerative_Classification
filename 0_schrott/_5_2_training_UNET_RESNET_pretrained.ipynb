{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import mlflow\n",
    "import mlflow.pytorch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    " # Import functions from the module\n",
    "import importlib\n",
    "import help_files._0_definitions \n",
    "import  help_files._1_visuals_script\n",
    "# import  help_files._01_load_data\n",
    " # Reload the module to apply the changes to the script\n",
    "importlib.reload(help_files._0_definitions)\n",
    "importlib.reload(help_files._1_visuals_script)\n",
    "# importlib.reload(help_files._01_load_data)\n",
    "import  help_files._1_visuals_script  as pauls_vs\n",
    "# Group by 'condition', 'level', and 'severity' and count occurrences\n",
    "from help_files._0_definitions import count_severity_by_condition_level \n",
    "# Define the path\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.width\", 1000)  # Set a large width to prevent line wrapping\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_definitions.py\") as file:\n",
    "    exec(file.read())\n",
    "    ### In definitions are all the functions that are used in the notebook and globals\n",
    "with open(\"help_files/_0_run_definitions.py\") as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames have been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "file_names = [\"train_df_3_cat.csv\", \"test_df_3_cat.csv\"]\n",
    "# Load the data from the CSV files\n",
    "dataframes = [pd.read_csv(data_path_vor / file_name) for file_name in file_names]\n",
    "# Unpack the dataframes into separate variables\n",
    "train_df, test_df = dataframes\n",
    "\n",
    "print(\"DataFrames have been loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:red\"> this is a small sample : 48692</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining small sample vs. end smaple\n",
    "whole_data_set = False\n",
    "# end sample or small sample    \n",
    "if whole_data_set == True:\n",
    "    print(\"Using the whole data set\")\n",
    "else:\n",
    "    train_df = train_df.sample(n=100, random_state=RSEED)\n",
    "    test_df = test_df.sample(n=5000, random_state=RSEED)\n",
    "    display(Markdown('<span style=\"color:red\"> this is a small sample : 48692</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new code l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>severity</th>\n",
       "      <th>condition</th>\n",
       "      <th>level</th>\n",
       "      <th>series_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>image_path</th>\n",
       "      <th>missing_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>684747623</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>3.273038e+09</td>\n",
       "      <td>370.053272</td>\n",
       "      <td>506.149133</td>\n",
       "      <td>data/train_images_origin/684747623/774494956/7...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>2325650566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>2.076691e+09</td>\n",
       "      <td>293.720819</td>\n",
       "      <td>472.222032</td>\n",
       "      <td>data/train_images_origin/2325650566/20477869/5...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>624881903</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>1.233162e+09</td>\n",
       "      <td>375.776524</td>\n",
       "      <td>475.359848</td>\n",
       "      <td>data/train_images_origin/624881903/1233161684/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9120</th>\n",
       "      <td>3221995449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>2.156977e+08</td>\n",
       "      <td>231.017145</td>\n",
       "      <td>348.249377</td>\n",
       "      <td>data/train_images_origin/3221995449/215697714/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>117720278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>4.077719e+09</td>\n",
       "      <td>250.755900</td>\n",
       "      <td>308.758838</td>\n",
       "      <td>data/train_images_origin/117720278/2514759683/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>701848105</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>3.312375e+09</td>\n",
       "      <td>265.501177</td>\n",
       "      <td>347.109288</td>\n",
       "      <td>data/train_images_origin/701848105/759243276/1...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9836</th>\n",
       "      <td>3473524025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>7.462082e+08</td>\n",
       "      <td>158.934938</td>\n",
       "      <td>201.354724</td>\n",
       "      <td>data/train_images_origin/3473524025/1697065269...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11351</th>\n",
       "      <td>4030506789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>2.829807e+09</td>\n",
       "      <td>262.626703</td>\n",
       "      <td>316.686648</td>\n",
       "      <td>data/train_images_origin/4030506789/2653665985...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>2383836357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>2.176104e+08</td>\n",
       "      <td>177.032086</td>\n",
       "      <td>242.310160</td>\n",
       "      <td>data/train_images_origin/2383836357/217610363/...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>1408642922</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>l5/s1</td>\n",
       "      <td>6.793452e+08</td>\n",
       "      <td>247.450382</td>\n",
       "      <td>303.877863</td>\n",
       "      <td>data/train_images_origin/1408642922/2290485467...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         study_id  severity  condition  level     series_id           x           y                                         image_path  missing_image\n",
       "1935    684747623       2.0          0  l5/s1  3.273038e+09  370.053272  506.149133  data/train_images_origin/684747623/774494956/7...          False\n",
       "6494   2325650566       1.0          0  l5/s1  2.076691e+09  293.720819  472.222032  data/train_images_origin/2325650566/20477869/5...          False\n",
       "1720    624881903       2.0          0  l5/s1  1.233162e+09  375.776524  475.359848  data/train_images_origin/624881903/1233161684/...          False\n",
       "9120   3221995449       1.0          0  l5/s1  2.156977e+08  231.017145  348.249377  data/train_images_origin/3221995449/215697714/...          False\n",
       "360     117720278       0.0          0  l5/s1  4.077719e+09  250.755900  308.758838  data/train_images_origin/117720278/2514759683/...          False\n",
       "...           ...       ...        ...    ...           ...         ...         ...                                                ...            ...\n",
       "2019    701848105       2.0          0  l5/s1  3.312375e+09  265.501177  347.109288  data/train_images_origin/701848105/759243276/1...          False\n",
       "9836   3473524025       0.0          0  l5/s1  7.462082e+08  158.934938  201.354724  data/train_images_origin/3473524025/1697065269...          False\n",
       "11351  4030506789       0.0          0  l5/s1  2.829807e+09  262.626703  316.686648  data/train_images_origin/4030506789/2653665985...          False\n",
       "6702   2383836357       0.0          0  l5/s1  2.176104e+08  177.032086  242.310160  data/train_images_origin/2383836357/217610363/...          False\n",
       "4029   1408642922       1.0          0  l5/s1  6.793452e+08  247.450382  303.877863  data/train_images_origin/1408642922/2290485467...          False\n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [8, 8] and output size of torch.Size([4, 4, 4, 4]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 167\u001b[0m\n\u001b[0;32m    165\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    166\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 167\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    169\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\HP1\\Desktop\\Spiced\\capstone-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP1\\Desktop\\Spiced\\capstone-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[13], line 52\u001b[0m, in \u001b[0;36mUNetWithResNet50.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Decoder part with upsampling\u001b[39;00m\n\u001b[0;32m     51\u001b[0m up4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv4(resnet_out)\n\u001b[1;32m---> 52\u001b[0m up4 \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Upsample to the original size\u001b[39;00m\n\u001b[0;32m     53\u001b[0m up3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv3(up4)\n\u001b[0;32m     54\u001b[0m up3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(up3, size\u001b[38;5;241m=\u001b[39mup4\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\HP1\\Desktop\\Spiced\\capstone-project\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:4453\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   4452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m-> 4453\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4454\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4455\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4456\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4458\u001b[0m         )\n\u001b[0;32m   4459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m   4460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [8, 8] and output size of torch.Size([4, 4, 4, 4]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define U-Net with ResNet-50 as the feature extractor\n",
    "class UNetWithResNet50(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetWithResNet50, self).__init__()\n",
    "\n",
    "        # Load a pre-trained ResNet-50 model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Modify the first convolution layer to accept the input with the desired number of channels\n",
    "        self.resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        # Remove the fully connected layers from ResNet-50\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "\n",
    "        # Decoder part for upsampling and creating the final output\n",
    "        self.upconv4 = self.upconv_block(2048, 1024)\n",
    "        self.upconv3 = self.upconv_block(1024, 512)\n",
    "        self.upconv2 = self.upconv_block(512, 256)\n",
    "        self.upconv1 = self.upconv_block(256, 128)\n",
    "\n",
    "        # Final convolution for regression\n",
    "        self.final_conv = nn.Conv2d(128, out_channels, kernel_size=1)\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the ResNet-50 feature extractor\n",
    "        resnet_out = self.resnet(x)\n",
    "\n",
    "        # Decoder part with upsampling\n",
    "        up4 = self.upconv4(resnet_out)\n",
    "        up4 = F.interpolate(up4, size=resnet_out.shape[2:] * 2, mode='bilinear', align_corners=False)  # Upsample to the original size\n",
    "        up3 = self.upconv3(up4)\n",
    "        up3 = F.interpolate(up3, size=up4.shape[2:] * 2, mode='bilinear', align_corners=False)\n",
    "        up2 = self.upconv2(up3)\n",
    "        up2 = F.interpolate(up2, size=up3.shape[2:] * 2, mode='bilinear', align_corners=False)\n",
    "        up1 = self.upconv1(up2)\n",
    "\n",
    "        # Final output\n",
    "        output = self.final_conv(up1)\n",
    "        output = F.adaptive_avg_pool2d(output, (1, 1))  # Reduce spatial dimensions\n",
    "        output = output.view(output.size(0), -1)  # Reshape to [batch_size, 2]\n",
    "        return output\n",
    "\n",
    "\n",
    "# Custom dataset for MRI localization\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        try:\n",
    "            dicom_image = pydicom.dcmread(img_path)\n",
    "            image_array = dicom_image.pixel_array\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load DICOM file at {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        image = Image.fromarray(image_array)\n",
    "        # Convert grayscale to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        x = torch.tensor(self.data.iloc[idx]['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[idx]['y'], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.tensor([x, y])\n",
    "\n",
    "\n",
    "# Data transformations (smaller image size of 128x128)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# DataLoader setup\n",
    "dataset = MRILocalizationDataset(data=train_df, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNetWithResNet50(in_channels=3, out_channels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set experiment in MLflow\n",
    "experiment_name = \"MRI_Localization_UNet_with_ResNet50\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Define an input example for MLflow\n",
    "input_example = torch.randn(1, 3, 128, 128, device=device)\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 5  # Number of epochs to wait for improvement before stopping\n",
    "diverge_count = 0\n",
    "max_diverge_count = 3  # Number of times validation loss can diverge before stopping\n",
    "stop_threshold = 0.1  # Threshold for validation loss divergence\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log experiment parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"model_architecture\", \"U-Net with ResNet-50 for Localization\")\n",
    "    mlflow.log_param(\"output_coordinates\", \"x, y\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            if inputs is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "\n",
    "        # Validation loss calculation (this should happen after training)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                if inputs is None or targets is None:\n",
    "                    continue\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Check if validation loss improves and save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter if validation loss improves\n",
    "            best_model_path = f\"best_model_weights_epoch_{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            mlflow.log_artifact(best_model_path)  # Log the best model weights\n",
    "            os.remove(best_model_path)  # Optionally, delete the local file after logging\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to lack of validation loss improvement.\")\n",
    "                break\n",
    "\n",
    "        # Early stopping based on validation loss divergence (optional)\n",
    "        if val_loss > best_val_loss * (1 + stop_threshold):\n",
    "            diverge_count += 1\n",
    "            if diverge_count >= max_diverge_count:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to validation loss divergence.\")\n",
    "                break\n",
    "        else:\n",
    "            diverge_count = 0\n",
    "\n",
    "    # Final logging\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "    print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Training Losses: {train_losses}\")\n",
    "    print(f\"Validation Losses: {val_losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ms\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define U-Net model with reduced complexity\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Downsampling (Encoder) with reduced filter sizes\n",
    "        self.enc1 = self.conv_block(in_channels, 32)\n",
    "        self.enc2 = self.conv_block(32, 64)\n",
    "        self.enc3 = self.conv_block(64, 128)\n",
    "        self.enc4 = self.conv_block(128, 256)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # Upsampling (Decoder)\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up3 = self.upconv_block(256, 128)\n",
    "        self.up2 = self.upconv_block(128, 64)\n",
    "        self.up1 = self.upconv_block(64, 32)\n",
    "\n",
    "        # Final convolution for regression\n",
    "        self.final_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "\n",
    "        # Decoder path with upsampling and skip connections\n",
    "        up4 = F.interpolate(self.up4(bottleneck), size=enc4.shape[2:]) + enc4\n",
    "        up3 = F.interpolate(self.up3(up4), size=enc3.shape[2:]) + enc3\n",
    "        up2 = F.interpolate(self.up2(up3), size=enc2.shape[2:]) + enc2\n",
    "        up1 = F.interpolate(self.up1(up2), size=enc1.shape[2:]) + enc1\n",
    "\n",
    "        # Final output\n",
    "        output = self.final_conv(up1)\n",
    "        output = F.adaptive_avg_pool2d(output, (1, 1))  # Reduce spatial dimensions\n",
    "        output = output.view(output.size(0), -1)  # Reshape to [batch_size, 2]\n",
    "        return output\n",
    "\n",
    "# Custom dataset for MRI localization\n",
    "class MRILocalizationDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        try:\n",
    "            dicom_image = pydicom.dcmread(img_path)\n",
    "            image_array = dicom_image.pixel_array\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load DICOM file at {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        image = Image.fromarray(image_array)\n",
    "        # Convert grayscale to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        x = torch.tensor(self.data.iloc[idx]['x'], dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[idx]['y'], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.tensor([x, y])\n",
    "\n",
    "# Data transformations (smaller image size of 128x128)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# DataLoader setup\n",
    "dataset = MRILocalizationDataset(data=train_df, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Model initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(in_channels=3, out_channels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set experiment in MLflow\n",
    "experiment_name = \"MRI_Localization_UNet\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Define an input example for MLflow\n",
    "input_example = torch.randn(1, 3, 128, 128, device=device)\n",
    "\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 5  # Number of epochs to wait for improvement before stopping\n",
    "diverge_count = 0\n",
    "max_diverge_count = 3  # Number of times validation loss can diverge before stopping\n",
    "stop_threshold = 0.1  # Threshold for validation loss divergence\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log experiment parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"model_architecture\", \"U-Net for Localization\")\n",
    "    mlflow.log_param(\"output_coordinates\", \"x, y\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            if inputs is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "\n",
    "        # Validation loss calculation (this should happen after training)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                if inputs is None or targets is None:\n",
    "                    continue\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Check if validation loss improves and save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter if validation loss improves\n",
    "            best_model_path = f\"best_model_weights_epoch_{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            mlflow.log_artifact(best_model_path)  # Log the best model weights\n",
    "            os.remove(best_model_path)  # Optionally, delete the local file after logging\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to lack of validation loss improvement.\")\n",
    "                break\n",
    "\n",
    "        # Early stopping based on validation loss divergence (optional)\n",
    "        if val_loss > best_val_loss * (1 + stop_threshold):\n",
    "            diverge_count += 1\n",
    "            if diverge_count >= max_diverge_count:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to validation loss divergence.\")\n",
    "                break\n",
    "        else:\n",
    "            diverge_count = 0  # Reset diverge count if validation loss doesn't diverge\n",
    "\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}')            \n",
    "\n",
    "        # Log metrics at each epoch\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "    # Log the final model\n",
    "    input_example_np = input_example.cpu().numpy()  # Convert tensor to numpy array\n",
    "    mlflow.pytorch.log_model(model, \"final_model\", input_example=input_example_np)\n",
    "\n",
    "\n",
    "    # Plot and save the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(num_epochs), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    mlflow.log_artifact(\"loss_curve.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\HP1\\\\Desktop\\\\Spiced\\\\capstone-project\\\\path_to_new_image.dcm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     test_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_to_new_image.dcm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     test_image \u001b[38;5;241m=\u001b[39m transform(test_image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     predicted_coords \u001b[38;5;241m=\u001b[39m model(test_image)\n",
      "File \u001b[1;32mc:\\Users\\HP1\\Desktop\\Spiced\\capstone-project\\.venv\\Lib\\site-packages\\PIL\\Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\HP1\\\\Desktop\\\\Spiced\\\\capstone-project\\\\path_to_new_image.dcm'"
     ]
    }
   ],
   "source": [
    "# Inference with disabled gradient tracking\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_image = Image.open(\"path_to_new_image.dcm\")\n",
    "    test_image = transform(test_image).unsqueeze(0)  # Add batch dimension\n",
    "    predicted_coords = model(test_image)\n",
    "    print(f\"Predicted x, y coordinates: {predicted_coords}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
